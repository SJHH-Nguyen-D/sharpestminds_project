{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_sharpestminds_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWib3nKlYsz",
        "colab_type": "text"
      },
      "source": [
        "# Employee Performance Data Modelling and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLYxlS0_b0Ur",
        "colab_type": "text"
      },
      "source": [
        "## About\n",
        "We try to predict employee performance scores based 300+ employee demographic characteristics. The dataset in question was acquired as part of a homework assignment and comprises of 20,000 employees and 379 employee characteristics, 1 target variable (employee performance score).\n",
        "\n",
        "This note book details the loading, preprocessing, sampling, feature selection, modeling and write up of the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV4K3c0-X3_Y",
        "colab_type": "text"
      },
      "source": [
        "Setting up the notebook settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGO5nOCvWoqD",
        "colab_type": "code",
        "outputId": "dab63017-0ae2-4069-fdfa-288f491eafb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "!--NotebookApp.iopub_data_rate_limit=1e11"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: --: invalid option\n",
            "Usage:\t/bin/bash [GNU long option] [option] ...\n",
            "\t/bin/bash [GNU long option] [option] script-file ...\n",
            "GNU long options:\n",
            "\t--debug\n",
            "\t--debugger\n",
            "\t--dump-po-strings\n",
            "\t--dump-strings\n",
            "\t--help\n",
            "\t--init-file\n",
            "\t--login\n",
            "\t--noediting\n",
            "\t--noprofile\n",
            "\t--norc\n",
            "\t--posix\n",
            "\t--rcfile\n",
            "\t--restricted\n",
            "\t--verbose\n",
            "\t--version\n",
            "Shell options:\n",
            "\t-ilrsD or -c command or -O shopt_option\t\t(invocation only)\n",
            "\t-abefhkmnptuvxBCHP or -o option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky7WF9AuBvkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AW_r0tMklda",
        "colab_type": "text"
      },
      "source": [
        "# Project Flow Overview\n",
        "\n",
        "1. Load Data\n",
        "2. Cursory Description of Data\n",
        "3. EDA\n",
        "4. Feature Selection/ Extraction\n",
        "5. Modelling\n",
        "6. Prediction\n",
        "7. Optimization\n",
        "8. Modelling\n",
        "9. End Point\n",
        "10. Conclusion and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZdh_qvRlDW0",
        "colab_type": "text"
      },
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9aXoJbOJdwo",
        "colab_type": "code",
        "outputId": "1251a507-8b53-4def-90cb-70c4146296ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1Y4_ViMIGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get colmumn names for main dataset with key from other dataset\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.options.display.max_seq_items = 2000\n",
        "import numpy as np\n",
        "\n",
        "col_names = pd.read_csv(\"/content/drive/My Drive/sharpestminds_dataset/CodeBook-SELECT.csv\")\n",
        "col_names = col_names.loc[:, \"VarName\"].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-aic2XAMYZ4",
        "colab_type": "code",
        "outputId": "b12cea68-d349-44b9-c694-c45138754640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        }
      },
      "source": [
        "# load in the actual dataset\n",
        "df = pd.read_csv(\"/content/drive/My Drive/sharpestminds_dataset/hw5-trainingset-cl3770.csv\", header=\"infer\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cntryid</th>\n",
              "      <th>cntryid_e</th>\n",
              "      <th>age_r</th>\n",
              "      <th>gender_r</th>\n",
              "      <th>computerexperience</th>\n",
              "      <th>nativespeaker</th>\n",
              "      <th>edlevel3</th>\n",
              "      <th>monthlyincpr</th>\n",
              "      <th>yearlyincpr</th>\n",
              "      <th>lng_home</th>\n",
              "      <th>cnt_h</th>\n",
              "      <th>cnt_brth</th>\n",
              "      <th>reg_tl2</th>\n",
              "      <th>lng_bq</th>\n",
              "      <th>lng_ci</th>\n",
              "      <th>yrsqual</th>\n",
              "      <th>yrsqual_t</th>\n",
              "      <th>yrsget</th>\n",
              "      <th>vet</th>\n",
              "      <th>ctryqual</th>\n",
              "      <th>birthrgn</th>\n",
              "      <th>nativelang</th>\n",
              "      <th>ctryrgn</th>\n",
              "      <th>imyrs</th>\n",
              "      <th>imyrs_c</th>\n",
              "      <th>imyrcat</th>\n",
              "      <th>ageg5lfs</th>\n",
              "      <th>ageg10lfs</th>\n",
              "      <th>ageg10lfs_t</th>\n",
              "      <th>edcat8</th>\n",
              "      <th>edcat7</th>\n",
              "      <th>edcat6</th>\n",
              "      <th>leaver1624</th>\n",
              "      <th>leavedu</th>\n",
              "      <th>fe12</th>\n",
              "      <th>aetpop</th>\n",
              "      <th>faet12</th>\n",
              "      <th>faet12jr</th>\n",
              "      <th>faet12njr</th>\n",
              "      <th>nfe12</th>\n",
              "      <th>...</th>\n",
              "      <th>v253</th>\n",
              "      <th>v132</th>\n",
              "      <th>v284</th>\n",
              "      <th>v267</th>\n",
              "      <th>v260</th>\n",
              "      <th>v26</th>\n",
              "      <th>v171</th>\n",
              "      <th>v14</th>\n",
              "      <th>v7</th>\n",
              "      <th>v240</th>\n",
              "      <th>v186</th>\n",
              "      <th>v162</th>\n",
              "      <th>v149</th>\n",
              "      <th>v228</th>\n",
              "      <th>v28</th>\n",
              "      <th>v237</th>\n",
              "      <th>v280</th>\n",
              "      <th>v175</th>\n",
              "      <th>v288</th>\n",
              "      <th>v15</th>\n",
              "      <th>v208</th>\n",
              "      <th>v43</th>\n",
              "      <th>v27</th>\n",
              "      <th>v114</th>\n",
              "      <th>v191</th>\n",
              "      <th>v170</th>\n",
              "      <th>v65</th>\n",
              "      <th>v57</th>\n",
              "      <th>v177</th>\n",
              "      <th>v69</th>\n",
              "      <th>v85</th>\n",
              "      <th>v50</th>\n",
              "      <th>v89</th>\n",
              "      <th>v127</th>\n",
              "      <th>v239</th>\n",
              "      <th>v224</th>\n",
              "      <th>v71</th>\n",
              "      <th>v105</th>\n",
              "      <th>row</th>\n",
              "      <th>uni</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Canada</td>\n",
              "      <td>Canada (English)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Medium</td>\n",
              "      <td>50 to less than 75</td>\n",
              "      <td>50 to less than 75</td>\n",
              "      <td>999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99999</td>\n",
              "      <td>eng</td>\n",
              "      <td>eng</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test language same as native language</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Non-immigrants</td>\n",
              "      <td>Aged 25-29</td>\n",
              "      <td>25-34</td>\n",
              "      <td>25-34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Upper secondary (ISCED 3A-B, C long)</td>\n",
              "      <td>Upper secondary (ISCED 3A-B, C long)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Participated in FE</td>\n",
              "      <td>AET population</td>\n",
              "      <td>Participated in formal AET</td>\n",
              "      <td>Participated in formal AET for JR reasons</td>\n",
              "      <td>Did not participate in FE for NJR reasons</td>\n",
              "      <td>Did not participate in NFE</td>\n",
              "      <td>...</td>\n",
              "      <td>Never</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>Never</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>Never</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Never</td>\n",
              "      <td>Never</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>Never</td>\n",
              "      <td>Disagree</td>\n",
              "      <td>Neither agree nor disagree</td>\n",
              "      <td>Agree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9999.0</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>9999</td>\n",
              "      <td>9999.0</td>\n",
              "      <td>80219</td>\n",
              "      <td>cl3770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>England (UK)</td>\n",
              "      <td>60.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>High</td>\n",
              "      <td>75 to less than 90</td>\n",
              "      <td>75 to less than 90</td>\n",
              "      <td>eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
              "      <td>UKJ</td>\n",
              "      <td>eng</td>\n",
              "      <td>eng</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>Test language same as native language</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Non-immigrants</td>\n",
              "      <td>Aged 60-65</td>\n",
              "      <td>55 plus</td>\n",
              "      <td>55 plus</td>\n",
              "      <td>Tertiary – professional degree (ISCED 5B)</td>\n",
              "      <td>Tertiary – professional degree (ISCED 5B)</td>\n",
              "      <td>Tertiary – professional degree (ISCED 5B)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Did not participate in FE</td>\n",
              "      <td>AET population</td>\n",
              "      <td>Did not participate in formal AET</td>\n",
              "      <td>Did not participate in formal AET for JR reasons</td>\n",
              "      <td>Did not participate in FE for NJR reasons</td>\n",
              "      <td>Participated in NFE</td>\n",
              "      <td>...</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Rarely</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>Never</td>\n",
              "      <td>Agree</td>\n",
              "      <td>Strongly agree</td>\n",
              "      <td>Agree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3118.0</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>2520</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>44314</td>\n",
              "      <td>cl3770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>England (UK)</td>\n",
              "      <td>31.0</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>High</td>\n",
              "      <td>75 to less than 90</td>\n",
              "      <td>75 to less than 90</td>\n",
              "      <td>eng</td>\n",
              "      <td>NaN</td>\n",
              "      <td>United Kingdom of Great Britain and Northern I...</td>\n",
              "      <td>UKI</td>\n",
              "      <td>eng</td>\n",
              "      <td>eng</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>Test language same as native language</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Non-immigrants</td>\n",
              "      <td>Aged 30-34</td>\n",
              "      <td>25-34</td>\n",
              "      <td>25-34</td>\n",
              "      <td>Tertiary - bachelor/master/research degree (IS...</td>\n",
              "      <td>Tertiary - bachelor/master/research degree (IS...</td>\n",
              "      <td>Tertiary - bachelor/master/research degree (IS...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Participated in FE</td>\n",
              "      <td>AET population</td>\n",
              "      <td>Participated in formal AET</td>\n",
              "      <td>Participated in formal AET for JR reasons</td>\n",
              "      <td>Did not participate in FE for NJR reasons</td>\n",
              "      <td>Participated in NFE</td>\n",
              "      <td>...</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Rarely</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Never</td>\n",
              "      <td>Never</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Disagree</td>\n",
              "      <td>Strongly agree</td>\n",
              "      <td>Agree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2421.0</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>3510</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>91755</td>\n",
              "      <td>cl3770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Norway</td>\n",
              "      <td>Norway</td>\n",
              "      <td>33.0</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>High</td>\n",
              "      <td>25 to less than 50</td>\n",
              "      <td>25 to less than 50</td>\n",
              "      <td>rus</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99999</td>\n",
              "      <td>eng</td>\n",
              "      <td>nor</td>\n",
              "      <td>18.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>Central Asia</td>\n",
              "      <td>Test language not same as native language</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0-5 years</td>\n",
              "      <td>In host country 5 or fewer years</td>\n",
              "      <td>Aged 30-34</td>\n",
              "      <td>25-34</td>\n",
              "      <td>25-34</td>\n",
              "      <td>Tertiary – master degree (ISCED 5A)</td>\n",
              "      <td>Tertiary – master/research degree (ISCED 5A/6)</td>\n",
              "      <td>Tertiary – master/research degree (ISCED 5A/6)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>31.0</td>\n",
              "      <td>Did not participate in FE</td>\n",
              "      <td>AET population</td>\n",
              "      <td>Did not participate in formal AET</td>\n",
              "      <td>Did not participate in formal AET for JR reasons</td>\n",
              "      <td>Did not participate in FE for NJR reasons</td>\n",
              "      <td>Participated in NFE</td>\n",
              "      <td>...</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To some extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>Less than once a month</td>\n",
              "      <td>Neither agree nor disagree</td>\n",
              "      <td>Disagree</td>\n",
              "      <td>Agree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>334.0</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>82</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>188729</td>\n",
              "      <td>cl3770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>United States</td>\n",
              "      <td>United States</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Medium</td>\n",
              "      <td>10 to less than 25</td>\n",
              "      <td>10 to less than 25</td>\n",
              "      <td>999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>eng</td>\n",
              "      <td>eng</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test language not same as native language</td>\n",
              "      <td>North America and Western Europe</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11-15 years</td>\n",
              "      <td>In host country more than 5 years</td>\n",
              "      <td>Aged 20-24</td>\n",
              "      <td>24 or less</td>\n",
              "      <td>24 or less</td>\n",
              "      <td>Upper secondary (ISCED 3A-B, C long)</td>\n",
              "      <td>Upper secondary (ISCED 3A-B, C long)</td>\n",
              "      <td>Upper secondary (ISCED 3A-B, C long)</td>\n",
              "      <td>Completed ISCED 3 or is still in education, ag...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Participated in FE</td>\n",
              "      <td>Excluded from AET population</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Did not participate in NFE</td>\n",
              "      <td>...</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Less than once a week but at least once a month</td>\n",
              "      <td>Never</td>\n",
              "      <td>Never</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Yes</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>At least once a week but not every day</td>\n",
              "      <td>Never</td>\n",
              "      <td>Never</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>To a very high extent</td>\n",
              "      <td>Never</td>\n",
              "      <td>Strongly disagree</td>\n",
              "      <td>Strongly agree</td>\n",
              "      <td>Strongly agree</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9999.0</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>9999</td>\n",
              "      <td>9996.0</td>\n",
              "      <td>38262</td>\n",
              "      <td>cl3770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 380 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          cntryid         cntryid_e  age_r  ...    v105     row     uni\n",
              "0          Canada  Canada (English)    NaN  ...  9999.0   80219  cl3770\n",
              "1  United Kingdom      England (UK)   60.0  ...  9996.0   44314  cl3770\n",
              "2  United Kingdom      England (UK)   31.0  ...  9996.0   91755  cl3770\n",
              "3          Norway            Norway   33.0  ...  9996.0  188729  cl3770\n",
              "4   United States     United States    NaN  ...  9996.0   38262  cl3770\n",
              "\n",
              "[5 rows x 380 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmvh5FescYh3",
        "colab_type": "text"
      },
      "source": [
        "### The dataset is a 93Mb csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyIvEzFWcORw",
        "colab_type": "code",
        "outputId": "e6bd603e-cdd1-49b9-a83c-8cb64ecaa0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!du -hs \"/content/drive/My Drive/sharpestminds_dataset/hw5-trainingset-cl3770.csv\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93M\t/content/drive/My Drive/sharpestminds_dataset/hw5-trainingset-cl3770.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVhwlVnWciux",
        "colab_type": "code",
        "outputId": "39f755cf-b751-426a-9e49-2b881fd4e38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "# we have a lot of data here\n",
        "print(\"{} features, {} observations\".format(df.shape[1], df.shape[0]))\n",
        "\n",
        "# we also have a lot of non-numeric data as well\n",
        "print(df.info())\n",
        "\n",
        "print(df.describe())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "380 features, 20000 observations\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20000 entries, 0 to 19999\n",
            "Columns: 380 entries, cntryid to uni\n",
            "dtypes: float64(80), int64(5), object(295)\n",
            "memory usage: 58.0+ MB\n",
            "None\n",
            "              age_r       yrsqual  ...          v105            row\n",
            "count  12118.000000  17417.000000  ...  19979.000000   20000.000000\n",
            "mean      39.339412     14.573549  ...   9833.138696   96825.751700\n",
            "std       11.004673      2.703482  ...   1009.110938   57590.955791\n",
            "min       16.000000      5.000000  ...     31.000000      98.000000\n",
            "25%       31.000000     12.000000  ...   9996.000000   46369.000000\n",
            "50%       38.000000     15.000000  ...   9996.000000   94729.000000\n",
            "75%       48.000000     16.000000  ...   9996.000000  147668.500000\n",
            "max       65.000000     22.000000  ...   9999.000000  197786.000000\n",
            "\n",
            "[8 rows x 85 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFmy5JVRiIiF",
        "colab_type": "code",
        "outputId": "912fd573-50d8-428c-ac4e-80ecd4dec351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# country based features\n",
        "country_features = ['ctryrgn', 'ctryqual', 'birthrgn', 'reg_tl2', 'cntryid_e', 'cntryid']\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@@@ ctryrgn @@@\n",
            "North America and Western Europe                13208\n",
            "Central and Eastern Europe                       3268\n",
            "East Asia and the Pacific (richer countries)     3036\n",
            "Latin America and the Caribbean                   344\n",
            "Name: ctryrgn, dtype: int64\n",
            "Number of unique values: 5\n",
            "\n",
            "\n",
            "@@@ ctryqual @@@\n",
            "North America and Western Europe                5236\n",
            "East Asia and the Pacific (richer countries)    3122\n",
            "Central and Eastern Europe                      2078\n",
            "East Asia and the Pacific (poorer countries)      16\n",
            "Latin America and the Caribbean                   10\n",
            "South and West Asia                                6\n",
            "Sub-Saharan Africa                                 3\n",
            "Arab States                                        2\n",
            "Central Asia                                       1\n",
            "Name: ctryqual, dtype: int64\n",
            "Number of unique values: 10\n",
            "\n",
            "\n",
            "@@@ birthrgn @@@\n",
            "North America and Western Europe                5439\n",
            "Central and Eastern Europe                      3324\n",
            "East Asia and the Pacific (richer countries)    3072\n",
            "Latin America and the Caribbean                  409\n",
            "East Asia and the Pacific (poorer countries)      85\n",
            "Arab States                                       60\n",
            "South and West Asia                               59\n",
            "Sub-Saharan Africa                                53\n",
            "Central Asia                                      16\n",
            "Name: birthrgn, dtype: int64\n",
            "Number of unique values: 10\n",
            "\n",
            "\n",
            "@@@ reg_tl2 @@@\n",
            "99999    5160\n",
            "JPC       629\n",
            "KR01      628\n",
            "JPF       224\n",
            "JPG       214\n",
            "NL3       206\n",
            "FR10      205\n",
            "UKJ       204\n",
            "ES30      197\n",
            "KR02      183\n",
            "JPD       172\n",
            "UKI       169\n",
            "CL13      168\n",
            "PL12      162\n",
            "UKD       161\n",
            "SG00      156\n",
            "UKH       154\n",
            "KR05      143\n",
            "BE2       143\n",
            "FR71      133\n",
            "JPH       126\n",
            "ES61      124\n",
            "JPJ       121\n",
            "ES51      119\n",
            "JPE       117\n",
            "RU01      115\n",
            "UKK       115\n",
            "NZ01      112\n",
            "RU74      110\n",
            "RU28      108\n",
            "KR03      101\n",
            "UKG       100\n",
            "UKF       100\n",
            "GR3        95\n",
            "IE02       93\n",
            "CZ06       91\n",
            "KR04       87\n",
            "NL4        87\n",
            "FR61       87\n",
            "PL22       87\n",
            "RU37       82\n",
            "PL51       80\n",
            "UKE        79\n",
            "NL2        78\n",
            "RU58       77\n",
            "ES52       76\n",
            "PL21       75\n",
            "RU40       73\n",
            "FR82       72\n",
            "RU56       71\n",
            "RU45       68\n",
            "FR62       68\n",
            "RU64       66\n",
            "FR24       66\n",
            "FR30       65\n",
            "FR51       64\n",
            "GR1        63\n",
            "JPB        61\n",
            "CZ01       60\n",
            "ES41       59\n",
            "IL04       58\n",
            "SK03       58\n",
            "RU39       56\n",
            "DK01       55\n",
            "RU41       55\n",
            "RU22       55\n",
            "ES11       52\n",
            "CZ03       50\n",
            "CL05       50\n",
            "FR22       49\n",
            "RU67       49\n",
            "SK02       49\n",
            "ES21       48\n",
            "SE11       48\n",
            "UKC        47\n",
            "SK04       46\n",
            "PL32       44\n",
            "RU50       44\n",
            "FR52       44\n",
            "RU65       44\n",
            "FR41       43\n",
            "PL41       43\n",
            "PL63       43\n",
            "EE00       43\n",
            "PL11       42\n",
            "PL31       42\n",
            "CZ08       42\n",
            "CZ07       41\n",
            "UKN        40\n",
            "SI02       40\n",
            "NL1        40\n",
            "CZ02       40\n",
            "FR81       38\n",
            "CZ05       37\n",
            "ES42       37\n",
            "CL08       36\n",
            "DK04       35\n",
            "PL61       33\n",
            "NZ02       32\n",
            "FR25       32\n",
            "SE23       32\n",
            "GR2        32\n",
            "RU15       31\n",
            "PL33       31\n",
            "SK01       31\n",
            "FR42       31\n",
            "IL05       30\n",
            "JPA        30\n",
            "ES53       29\n",
            "CL14       29\n",
            "RU19       29\n",
            "CZ04       28\n",
            "FR26       28\n",
            "ES12       28\n",
            "ES62       28\n",
            "ES13       27\n",
            "SI01       27\n",
            "IL06       25\n",
            "DK03       24\n",
            "CL01       24\n",
            "KR06       23\n",
            "RU08       23\n",
            "SE12       22\n",
            "FR72       21\n",
            "SE22       21\n",
            "FR23       21\n",
            "FR21       21\n",
            "IL02       20\n",
            "PL52       20\n",
            "IL03       20\n",
            "PL42       19\n",
            "RU27       19\n",
            "IE01       19\n",
            "ES70       19\n",
            "ES24       19\n",
            "PL34       19\n",
            "GR4        18\n",
            "RU16       18\n",
            "JPI        17\n",
            "LT09       17\n",
            "IL01       17\n",
            "ES22       16\n",
            "FR43       14\n",
            "PL62       14\n",
            "PL43       14\n",
            "FR53       12\n",
            "DK02       12\n",
            "CL07       11\n",
            "RU44       10\n",
            "SE21       10\n",
            "FR83        9\n",
            "LT07        9\n",
            "LT08        9\n",
            "CL06        8\n",
            "DK05        8\n",
            "LT03        8\n",
            "CL09        8\n",
            "LT02        8\n",
            "IL07        8\n",
            "CL02        8\n",
            "ES43        7\n",
            "RU54        7\n",
            "FR63        7\n",
            "SE31        7\n",
            "LT04        6\n",
            "LT10        5\n",
            "SE33        5\n",
            "LT06        4\n",
            "ES23        4\n",
            "KR07        4\n",
            "SE32        3\n",
            "LT05        2\n",
            "CL10        2\n",
            "ES63        2\n",
            "LT01        2\n",
            "Name: reg_tl2, dtype: int64\n",
            "Number of unique values: 176\n",
            "\n",
            "\n",
            "@@@ cntryid_e @@@\n",
            "United States            4061\n",
            "Germany                  2061\n",
            "Japan                    1711\n",
            "Russian Federation       1210\n",
            "Korea                    1169\n",
            "France                   1130\n",
            "England (UK)             1129\n",
            "Canada (English)         1079\n",
            "Spain                     891\n",
            "Italy                     806\n",
            "Poland                    768\n",
            "Turkey                    537\n",
            "Netherlands               411\n",
            "Czech Republic            389\n",
            "Chile                     344\n",
            "Greece                    208\n",
            "Finland                   207\n",
            "Canada (French)           194\n",
            "Austria                   186\n",
            "Slovak Republic           184\n",
            "Israel                    178\n",
            "Singapore                 156\n",
            "Sweden                    148\n",
            "New Zealand               144\n",
            "Flanders (Belgium)        143\n",
            "Denmark                   134\n",
            "Ireland                   112\n",
            "Norway                     89\n",
            "Lithuania                  70\n",
            "Slovenia                   67\n",
            "Estonia                    43\n",
            "Northern Ireland (UK)      40\n",
            "Name: cntryid_e, dtype: int64\n",
            "Number of unique values: 33\n",
            "\n",
            "\n",
            "@@@ cntryid @@@\n",
            "United States         4061\n",
            "Germany               2061\n",
            "Japan                 1711\n",
            "Canada                1274\n",
            "Russian Federation    1210\n",
            "Korea                 1169\n",
            "United Kingdom        1169\n",
            "France                1130\n",
            "Spain                  891\n",
            "Italy                  806\n",
            "Poland                 768\n",
            "Turkey                 537\n",
            "Netherlands            411\n",
            "Czech Republic         389\n",
            "Chile                  344\n",
            "Greece                 208\n",
            "Finland                207\n",
            "Austria                186\n",
            "Slovak Republic        184\n",
            "Israel                 178\n",
            "Singapore              156\n",
            "Sweden                 148\n",
            "New Zealand            144\n",
            "Belgium                143\n",
            "Denmark                134\n",
            "Ireland                112\n",
            "Norway                  89\n",
            "Lithuania               70\n",
            "Slovenia                67\n",
            "Estonia                 43\n",
            "Name: cntryid, dtype: int64\n",
            "Number of unique values: 30\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c99SaK94lqzu",
        "colab_type": "text"
      },
      "source": [
        "# 3. EDA - Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbBmc5nQ4kYu",
        "colab_type": "text"
      },
      "source": [
        "### We print out the the counts of observations with missing values in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eqs2evmNAQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "fc3f8498-bb35-4b3c-cd51-131a8149c7c2"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# There are a lot of missing values in this dataset\n",
        "print(df.isnull().sum().sort_values(ascending=False)[df.isnull().sum() > 1000][:25])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v262    20000\n",
            "v44     19997\n",
            "v76     19993\n",
            "v144    19992\n",
            "v199    19991\n",
            "v159    19985\n",
            "v10     19981\n",
            "v172    19977\n",
            "v110    19956\n",
            "v160    19955\n",
            "v100    19955\n",
            "v109    19954\n",
            "v75     19938\n",
            "v211    19938\n",
            "v215    19936\n",
            "v163    19935\n",
            "v220    19927\n",
            "v254    19923\n",
            "v38     19910\n",
            "v129    19910\n",
            "v266    19907\n",
            "v89     19864\n",
            "v127    19828\n",
            "v117    19793\n",
            "v287    19792\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoSrbh7Un5wJ",
        "colab_type": "text"
      },
      "source": [
        "## Missing Data Proportion by Columns\n",
        "We will want to drop a number of the columns have have a significant amount of its data missing. Some features have disproportionately more data missing than others and there will be no way to impute those features. In which case dropping those features might be the best course of action to clean up the dataset. Feature column values that have a proportion of their data missing equal to or greater than an arbitrary threshold will have their columns dropped. Afterwards, a row-wise operation of this procedure will be further applied to include cleaner samples in the dataset. \n",
        "________________\n",
        "\n",
        "We will examine to see which features reach this threshold requirement to be dropped before we do any such dropping. \n",
        "____________ \n",
        "\n",
        "Afterwards, we will want to smarly impute the remaining missing data points as much as we can."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-V0WcWv8hOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "894a3ba6-8de5-429e-dc70-508245bdd882"
      },
      "source": [
        "# features by % of missing data\n",
        "dropthreshold = 50\n",
        "\n",
        "pct_missing_list = list([col for col in df.columns if df[col].isnull().sum()/df.shape[0] * 100 >= dropthreshold])\n",
        "pm_list = []\n",
        "\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].isnull().sum()/df.shape[0] * 100 >= dropthreshold:\n",
        "        pm_list.append(df[col].isnull().sum()/df.shape[0] * 100)\n",
        "        \n",
        "missing_data_series = pd.DataFrame(pm_list, columns=['% Missing Data'], index=pct_missing_list).sort_values(by='% Missing Data', ascending=False)\n",
        "\n",
        "# get the data points in the dataframe that have >= 60% of their data missing\n",
        "df_col_drop_gtoe_60 = df[pct_missing_list]\n",
        "kept_feat_cols = list(set(df.columns) - set(pct_missing_list))\n",
        "reduced_cols_gtoe_60_missing_df = df[kept_feat_cols]\n",
        "\n",
        "print(\"The features with {}% or more of it's data missing will be dropped from the dataset.\".format(dropthreshold))\n",
        "print(\"The number of features of the dataset has been reduced by {}% after dropping the columns with a more than {}% of its data missing.\".format(reduced_cols_gtoe_60_missing_df.shape[1]/df.shape[1] * 100, dropthreshold))\n",
        "print(\"This is the size of the dataset before it was reduced: {}\".format(df.shape))\n",
        "print(\"The shape of the data that would be dropped with >= {}% of its data missing : {}\".format(dropthreshold, df_col_drop_gtoe_60.shape))\n",
        "print(\"This is the shape of the remaining dataset, after having dropped the data points with GTOE {}% of the data missing: {}, \" \\\n",
        "\"meaning that {} columns of the original dataframe will be dropped\".format(dropthreshold, reduced_cols_gtoe_60_missing_df.shape, len(df.columns) - len(kept_feat_cols)))\n",
        "\n",
        "# save the results to csv to look at later\n",
        "# missing_data_series.to_csv('./features_greater_{}_percent_missing_data_by_cols.csv'.format(dropthreshold))\n",
        "\n",
        "# write out the dataframe after having dropped the rows\n",
        "# reduced_cols_gtoe_60_missing_df.to_csv('./reduced_cols_gtoe_{}_missing_df.csv'.format(dropthreshold))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The features with 50% or more of it's data missing will be dropped from the dataset.\n",
            "The number of features of the dataset has been reduced by 63.68421052631579% after dropping the columns with a more than 50% of its data missing.\n",
            "This is the size of the dataset before it was reduced: (20000, 380)\n",
            "The shape of the data that would be dropped with >= 50% of its data missing : (20000, 138)\n",
            "This is the shape of the remaining dataset, after having dropped the data points with GTOE 50% of the data missing: (20000, 242), meaning that 138 columns of the original dataframe will be dropped\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7KwoFyV4DyC",
        "colab_type": "text"
      },
      "source": [
        "## Missing Data Proportion By Row \n",
        "An alternative strategy would be just to drop all of the rows with missing values. This step requires a little more wor as we will have more than 0 data points that have missing values and require imputation of missing values.\n",
        "\n",
        "\n",
        "Lets first drop any data ROWS with 50% or more missing values and see what we end up with.\n",
        "\n",
        "________________\n",
        "\n",
        "\n",
        "Just a refresher on how to how to conditionally select rows from a data frame:\n",
        "\n",
        "To select rows whose column value equals a scalar, some_value, use ==:\n",
        "df.loc[df['column_name'] == some_value]\n",
        "\n",
        "\n",
        "To select rows whose column value is in an iterable, some_values, use isin:\n",
        "df.loc[df['column_name'].isin(some_values)]\n",
        "\n",
        "Combine multiple conditions with &:\n",
        "df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3PETq5SC7wr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "59cfbe6d-e7bd-4490-cda0-519e6b77faee"
      },
      "source": [
        "# Data points with percentage of data missing and greater will be dropped from the dataset\n",
        "dropthreshold = 20\n",
        "\n",
        "# get the data points in the dataframe that have >= 30% of their data missing\n",
        "df_row_gtoe_20_missing = reduced_cols_gtoe_60_missing_df.loc[(reduced_cols_gtoe_60_missing_df.isnull().sum(axis=1)/reduced_cols_gtoe_60_missing_df.shape[1]*100 >= dropthreshold)]\n",
        "reduced_row_gtoe_20_missing = reduced_cols_gtoe_60_missing_df.loc[(reduced_cols_gtoe_60_missing_df.isnull().sum(axis=1)/reduced_cols_gtoe_60_missing_df.shape[1]*100 <= dropthreshold)]\n",
        "\n",
        "print(\"The data points with {}% or more of it's data missing will be dropped from the dataset.\".format(dropthreshold))\n",
        "print(\"The size of the dataset has been reduced by {}% after dropping the rows with {}% of its data missing.\".format(df_row_gtoe_20_missing.shape[0]/reduced_cols_gtoe_60_missing_df.shape[0] * 100, dropthreshold))\n",
        "print(\"The size of the dataset remaining after the missing data points have been dropped {}%.\".format(reduced_row_gtoe_20_missing.shape[0]/reduced_cols_gtoe_60_missing_df.shape[0] * 100))\n",
        "print(\"This is the size of the dataset before it was reduced: {}\".format(reduced_cols_gtoe_60_missing_df.shape))\n",
        "print(\"The shape of the data that would be dropped with >= {}% of its data missing : {}\".format(dropthreshold, df_row_gtoe_20_missing.shape))\n",
        "print(\"This is the shape of the remaining dataset, after having dropped the data points with GTOE {}% of the data missing: {}\".format(dropthreshold, reduced_row_gtoe_20_missing.shape))\n",
        "print(\"As a result, {} data points have been dropped due to insufficient data\".format(reduced_cols_gtoe_60_missing_df.shape[0] - reduced_row_gtoe_20_missing.shape[0]))\n",
        "\n",
        "# give the new dataframe a better name\n",
        "final_kept_df = reduced_row_gtoe_20_missing\n",
        "\n",
        "# save the dataframe to disk after the processing\n",
        "# final_kept_df.to_csv('./final_kept_df.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data points with 20% or more of it's data missing will be dropped from the dataset.\n",
            "The size of the dataset has been reduced by 5.59% after dropping the rows with 20% of its data missing.\n",
            "The size of the dataset remaining after the missing data points have been dropped 94.41000000000001%.\n",
            "This is the size of the dataset before it was reduced: (20000, 242)\n",
            "The shape of the data that would be dropped with >= 20% of its data missing : (1118, 242)\n",
            "This is the shape of the remaining dataset, after having dropped the data points with GTOE 20% of the data missing: (18882, 242)\n",
            "As a result, 1118 data points have been dropped due to insufficient data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6RsGfvzzPce",
        "colab_type": "text"
      },
      "source": [
        "## Missing Data by Data Type\n",
        "We can see that there are a considerable amount of missing values for this dataset. We will have to dig in to understand which features would have missing values, and can we come up with a strategy to handle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAGegy6qZyLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d173b56-7035-4f5f-c568-fe0708b7be85"
      },
      "source": [
        "# separate out the numeric and categorical variables to see how much of each are missing\n",
        "\n",
        "numeric_df = final_kept_df._get_numeric_data()\n",
        "categorical_df = final_kept_df.select_dtypes(exclude = [int, float])\n",
        "\n",
        "print(\"Number of Numeric Features: {}\".format(numeric_df.shape[1]))\n",
        "print(\"Number of Categorical Features: {}\".format(categorical_df.shape[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Numeric Features: 37\n",
            "Number of Categorical Features: 205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc54cbPazRx-",
        "colab_type": "text"
      },
      "source": [
        "## Numeric Feature Missing Values\n",
        "\n",
        "It still appears that after dropping the data points and features with a majority of their data points missing, that about a third of the numeric features containg NaN/missing values in more than 20% of their data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83BqJ9XwGdyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "c7dbfc0d-a5da-4c27-c2f7-7c3f11467c03"
      },
      "source": [
        "# take a look at the missing values from the numeric features dataframe\n",
        "print(\"Proportionally, numeric features contribute {0:.2f}% of the total features in the dataset\".format(numeric_df.shape[1]/final_kept_df.shape[1]*100))\n",
        "a = pd.Series(numeric_df.isnull().sum().sort_values(ascending=False)/(numeric_df.shape[0]) * 100)\n",
        "print(a[:12])\n",
        "a.plot(kind='bar')\n",
        "\n",
        "# 9 numeric features of 37 with more than 20% of it's data missing\n",
        "print(\"The number of numeric features with more than 20% of its values missing is: {}, which is {}% of all numeric features\".format(9, (9/numeric_df.shape[1]*100)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportionally, numeric features contribute 15.29% of the total features in the dataset\n",
            "leavedu      44.020761\n",
            "v202         38.894185\n",
            "age_r        38.703527\n",
            "v231         38.110370\n",
            "nfehrs       37.850863\n",
            "v272         37.612541\n",
            "v52          37.379515\n",
            "v33          37.337146\n",
            "v63          36.987607\n",
            "yrsget       17.847686\n",
            "yrsqual_t    13.372524\n",
            "yrsqual      12.832327\n",
            "dtype: float64\n",
            "The number of numeric features with more than 20% of its values missing is: 9, which is 24.324324324324326% of all numeric features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAFGCAYAAAB60WT1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ufjLzSH34y",
        "colab_type": "text"
      },
      "source": [
        "We are going to explore correlation/collinearity between the numeric features and drop redundant features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN8_YTMoMhfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ea7f149d-29ec-4d71-a599-ea9f1a9919b0"
      },
      "source": [
        "# categorical feature with numeric encoding\n",
        "\n",
        "cat_feat_with_num_encoding = ['isic2l', 'v224', 'isco1l', 'v105', 'isco1c', 'v239', 'isco2c', 'v105']\n",
        "for i in cat_feat_with_num_encoding:\n",
        "    print(\"This categorical feature with numeric encodings has {} unique values: {}\".format(i, len(numeric_df[i].unique())))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This categorical feature with numeric encodings has isic2l unique values: 37\n",
            "This categorical feature with numeric encodings has v224 unique values: 62\n",
            "This categorical feature with numeric encodings has isco1l unique values: 11\n",
            "This categorical feature with numeric encodings has v105 unique values: 58\n",
            "This categorical feature with numeric encodings has isco1c unique values: 15\n",
            "This categorical feature with numeric encodings has v239 unique values: 413\n",
            "This categorical feature with numeric encodings has isco2c unique values: 50\n",
            "This categorical feature with numeric encodings has v105 unique values: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsUsMHe6IPht",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Imputer Imputation for numeric features\n",
        "Some further processing can be done before the imputation step such as (dropping,casting, merging, etc). The next few segments of code will go through the next preprocessing steps as outlined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEL-EAuXNRLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we drop 'age_r' as there is another categorical feature containing the same demographic information with more completeness, with the tradeoff of numeric granularity\n",
        "numeric_df.drop(columns='age_r', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_A3eDpsNoZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we drop 'row' because this is a pretty useless category\n",
        "numeric_df.drop(columns='row', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJAWL1hje97q",
        "colab_type": "text"
      },
      "source": [
        "## Examination of the categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0u62ihTGiV-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "outputId": "25d7194f-9989-4948-c74b-a925b2299fef"
      },
      "source": [
        "# take a look at the missing values for categorical features dataframe\n",
        "print(\"Proportionally, categorical features contribute {0:.2f}% of the total features in the dataset\".format(categorical_df.shape[1]/final_kept_df.shape[1]*100))\n",
        "print((categorical_df.isnull().sum().sort_values(ascending=False)/(categorical_df.shape[0]) * 100 )[:20])\n",
        "b = pd.Series(categorical_df.isnull().sum().sort_values(ascending=False)/(categorical_df.shape[0]) * 100 )\n",
        "b.plot(kind='bar')\n",
        "\n",
        "# 17 features with more than 20% of its values missing of 205 features\n",
        "print(\"The number of categorical features with more than 20% of its values missing is: {}, which is {}% of all categorical features\".format(17, 17/categorical_df.shape[1]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportionally, categorical features contribute 84.71% of the total features in the dataset\n",
            "ctryqual          47.251350\n",
            "v291              45.689016\n",
            "v137              45.122339\n",
            "cnt_brth          43.745366\n",
            "v3                39.725665\n",
            "birthrgn          37.061752\n",
            "v269              35.833069\n",
            "v91               35.817180\n",
            "v47               33.095011\n",
            "v236              32.994386\n",
            "v289              27.846626\n",
            "v96               25.675246\n",
            "earnhrbonusdcl    23.111958\n",
            "earnhrdcl         22.863044\n",
            "v255              22.068637\n",
            "v212              21.787946\n",
            "reg_tl2           20.040250\n",
            "v62               18.011863\n",
            "earnflag          17.365745\n",
            "v8                17.222752\n",
            "dtype: float64\n",
            "The number of categorical features with more than 20% of its values missing is: 17, which is 8.292682926829269% of all categorical features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAFZCAYAAABnk7+FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVdPuwT73xDS",
        "colab_type": "text"
      },
      "source": [
        "## Unique categorical groupings in the categorical features \n",
        "\n",
        "Just to get a little perspective on how many categories there are for the dataset, we will print each of their unique values for the first several features. This will help us have an idea of how much the dataset will ballon once we onehot encode for each categorical feature value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfCdT__8nJA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "522e45ba-40c2-4d8d-a006-f6a0282555e4"
      },
      "source": [
        "# unique values for each categorical feature. need to know how what encoding scheme we should use\n",
        "\n",
        "num_cat_gtoe_10 = []\n",
        "for column in categorical_df.columns:\n",
        "    if len(categorical_df[column].unique()) >= 10:\n",
        "        print(f\"######### {column} ##########\")\n",
        "        print(categorical_df[column].unique()) # returns np.ndarray\n",
        "        print(\"Number of unique values: {}\\n\".format(len(categorical_df[column].unique())))\n",
        "        num_cat_gtoe_10.append(column)\n",
        "print(\"There are {} categorical columns with more than 10 categories\".format(len(num_cat_gtoe_10)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "######### isic2c ##########\n",
            "['9999' '25' '35' '82' '45' '47' '85' '55' '72' '84' '64' '41' '31' '26'\n",
            " '66' '01' '73' '43' '10' '91' '81' '29' '20' '9996' '71' '86' '46' '56'\n",
            " '49' '61' '28' '22' '21' '63' '30' '52' '69' '77' '62' '51' '42' '33'\n",
            " '14' 'K' '65' '88' '24' '80' '23' '32' '08' '79' '53' 'F' '58' 'C' '92'\n",
            " '75' '27' '9995' '02' '68' '18' '87' '70' '93' '15' 'T' '17' 'G' '16'\n",
            " '74' '9998' '03' '96' '12' nan '13' 'Q' '07' '94' '11' '95' '50' '60'\n",
            " '38' '59' 'A' '78' '06' '36' '09' '19' '39' 'J' '90' '37' 'H' '9997' '05'\n",
            " 'I' '97']\n",
            "Number of unique values: 102\n",
            "\n",
            "######### edcat8 ##########\n",
            "[nan 'Tertiary – professional degree (ISCED 5B)'\n",
            " 'Tertiary - bachelor/master/research degree (ISCED 5A/6)'\n",
            " 'Tertiary – master degree (ISCED 5A)'\n",
            " 'Upper secondary (ISCED 3A-B, C long)'\n",
            " 'Tertiary – bachelor degree (ISCED 5A)'\n",
            " 'Post-secondary, non-tertiary (ISCED 4A-B-C)'\n",
            " 'Tertiary – research degree (ISCED 6)'\n",
            " 'Lower secondary (ISCED 2, ISCED 3C short)'\n",
            " 'Primary or less (ISCED 1 or less)']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### earnhrdcl ##########\n",
            "['7th decile' nan 'Lowest decile' '2nd decile' 'Highest decile'\n",
            " '5th decile' '3rd decile' '4th decile' '6th decile' '9th decile'\n",
            " '8th decile']\n",
            "Number of unique values: 11\n",
            "\n",
            "######### lng_bq ##########\n",
            "['eng' 'rus' 'tur' 'swe' 'deu' 'kor' 'ita' 'pol' 'fra' 'jpn' 'nld' 'spa'\n",
            " 'slv' 'dan' 'slk' 'heb' 'ces' 'est' 'ell' 'fin' 'nor' 'ara' 'cat' 'lit'\n",
            " '999' 'glg' 'hun']\n",
            "Number of unique values: 27\n",
            "\n",
            "######### isic1c ##########\n",
            "['M' 'C' 'D' 'N' 'G' 'P' 'I' 'O' 'K' 'F' 'A' 'L' 'R' '9996' 'Q' 'H' 'J'\n",
            " '9999' 'B' '9995' 'T' '9998' 'S' 'E' '9997']\n",
            "Number of unique values: 25\n",
            "\n",
            "######### reg_tl2 ##########\n",
            "['99999' 'UKJ' 'UKI' nan 'RU40' 'SE11' 'KR01' 'IE02' 'SG00' 'PL22' 'UKD'\n",
            " 'RU28' 'FR71' 'JPH' 'UKH' 'NL3' 'FR10' 'CL09' 'JPD' 'KR03' 'KR05' 'ES12'\n",
            " 'KR04' 'RU74' 'FR22' 'KR02' 'FR81' 'SI01' 'FR43' 'DK04' 'PL11' 'UKG'\n",
            " 'UKF' 'ES13' 'BE2' 'RU64' 'SK02' 'PL12' 'RU37' 'IL04' 'ES22' 'FR21' 'JPG'\n",
            " 'CZ05' 'PL43' 'ES30' 'ES61' 'RU45' 'JPC' 'JPF' 'RU50' 'CL05' 'JPE' 'UKC'\n",
            " 'EE00' 'CZ08' 'GR3' 'FR51' 'JPJ' 'FR30' 'SK01' 'PL32' 'CL13' 'SE33'\n",
            " 'ES51' 'FR24' 'ES70' 'RU01' 'FR52' 'RU19' 'NL4' 'RU27' 'KR06' 'JPI'\n",
            " 'RU54' 'PL63' 'UKE' 'PL41' 'RU41' 'UKN' 'PL21' 'RU22' 'DK01' 'CZ01'\n",
            " 'PL61' 'RU39' 'CZ04' 'JPB' 'NL2' 'FR82' 'NZ01' 'PL52' 'ES52' 'FR23'\n",
            " 'CZ07' 'SE22' 'ES21' 'FR61' 'FR62' 'RU65' 'PL51' 'CL08' 'SK03' 'PL33'\n",
            " 'RU08' 'SE31' 'IL07' 'NZ02' 'IL02' 'PL31' 'RU58' 'PL42' 'ES42' 'GR2'\n",
            " 'ES53' 'SE12' 'UKK' 'IL05' 'RU56' 'CZ06' 'ES23' 'ES62' 'ES41' 'RU67'\n",
            " 'LT08' 'CZ03' 'GR1' 'ES24' 'IE01' 'PL34' 'JPA' 'LT04' 'FR42' 'FR53'\n",
            " 'FR41' 'NL1' 'SE21' 'RU44' 'SI02' 'PL62' 'LT05' 'ES11' 'LT09' 'CZ02'\n",
            " 'RU15' 'FR83' 'DK02' 'FR25' 'ES43' 'CL02' 'IL01' 'ES63' 'CL06' 'IL06'\n",
            " 'DK05' 'RU16' 'SK04' 'DK03' 'CL07' 'LT07' 'IL03' 'LT02' 'FR72' 'LT06'\n",
            " 'SE23' 'CL01' 'CL14' 'LT03' 'GR4' 'LT01' 'LT10' 'FR26' 'KR07' 'SE32'\n",
            " 'FR63' 'CL10']\n",
            "Number of unique values: 176\n",
            "\n",
            "######### birthrgn ##########\n",
            "[nan 'North America and Western Europe' 'Central Asia'\n",
            " 'Central and Eastern Europe'\n",
            " 'East Asia and the Pacific (richer countries)'\n",
            " 'East Asia and the Pacific (poorer countries)'\n",
            " 'Latin America and the Caribbean' 'Sub-Saharan Africa' 'Arab States'\n",
            " 'South and West Asia']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### cntryid_e ##########\n",
            "['Canada (English)' 'England (UK)' 'Norway' 'United States'\n",
            " 'Russian Federation' 'Turkey' 'Sweden' 'Germany' 'Korea' 'Ireland'\n",
            " 'Singapore' 'Italy' 'Poland' 'France' 'Japan' 'Netherlands' 'Chile'\n",
            " 'Spain' 'Slovenia' 'Denmark' 'Canada (French)' 'Flanders (Belgium)'\n",
            " 'Slovak Republic' 'Israel' 'Czech Republic' 'Estonia' 'Greece' 'Finland'\n",
            " 'Austria' 'Northern Ireland (UK)' 'New Zealand' 'Lithuania' nan]\n",
            "Number of unique values: 33\n",
            "\n",
            "######### v212 ##########\n",
            "[nan 'ISCED 3 (without distinction A-B-C, 2y+)' 'ISCED 5B'\n",
            " 'ISCED 5A, bachelor degree' 'ISCED 3A-B' 'ISCED 4C' 'ISCED 4A-B'\n",
            " 'ISCED 5A, master degree'\n",
            " 'ISCED 5A bachelor degree, 5A master degree, and 6 (without distinction)'\n",
            " 'ISCED 6' 'ISCED 1' 'ISCED 4 (without distinction A-B-C)' 'ISCED 2'\n",
            " 'No formal qualification or below ISCED 1' 'ISCED 3C 2 years or more'\n",
            " 'ISCED 3C shorter than 2 years']\n",
            "Number of unique values: 16\n",
            "\n",
            "######### earnmthalldcl ##########\n",
            "['6th decile' '2nd decile' '7th decile' '9th decile' '3rd decile'\n",
            " 'Highest decile' '5th decile' nan 'Lowest decile' '8th decile'\n",
            " '4th decile']\n",
            "Number of unique values: 11\n",
            "\n",
            "######### v59 ##########\n",
            "[nan 'ISCED 5B'\n",
            " 'ISCED 5A bachelor degree, 5A master degree, and 6 (without distinction)'\n",
            " 'ISCED 5A, master degree' 'ISCED 3 (without distinction A-B-C, 2y+)'\n",
            " 'ISCED 5A, bachelor degree' 'ISCED 3A-B' 'ISCED 4A-B'\n",
            " 'ISCED 3C 2 years or more' 'Foreign qualification' 'ISCED 6' 'ISCED 2'\n",
            " 'ISCED 4 (without distinction A-B-C)' 'ISCED 3C shorter than 2 years'\n",
            " 'ISCED 1' 'ISCED 4C' 'No formal qualification or below ISCED 1']\n",
            "Number of unique values: 17\n",
            "\n",
            "######### isic1l ##########\n",
            "['9999' '9996' 'C' 'I' 'P' 'K' '9995' 'G' 'S' 'Q' 'F' 'L' 'O' 'J' 'M' 'N'\n",
            " 'H']\n",
            "Number of unique values: 17\n",
            "\n",
            "######### v19 ##########\n",
            "['Aged 20-24' nan 'Aged 25-29' 'Aged 30-34' 'Aged 35-39' 'Aged 40-44'\n",
            " 'Aged 19 or younger' 'Aged 45-49' 'Aged 50-54' 'Aged 55 or older']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### lng_ci ##########\n",
            "['eng' 'nor' 'rus' 'tur' 'swe' 'deu' 'kor' 'ita' 'pol' 'fra' 'jpn' 'nld'\n",
            " 'spa' 'slv' 'dan' 'slk' 'heb' 'ces' 'est' 'ell' 'fin' 'cat' 'ara' 'lit'\n",
            " '999' 'glg' 'eus' nan 'hun']\n",
            "Number of unique values: 29\n",
            "\n",
            "######### earnhrbonusdcl ##########\n",
            "['7th decile' nan 'Lowest decile' '8th decile' '2nd decile'\n",
            " 'Highest decile' '6th decile' '3rd decile' '4th decile' '5th decile'\n",
            " '9th decile']\n",
            "Number of unique values: 11\n",
            "\n",
            "######### v71 ##########\n",
            "[9999.0 2520.0 3510.0 82.0 4711.0 8510.0 7210.0 3100.0 8521.0 8422.0\n",
            " 2660.0 721.0 149.0 113.0 4322.0 85.0 4751.0 1071.0 9102.0 6419.0 8411.0\n",
            " 4330.0 4730.0 9996.0 4773.0 7110.0 4100.0 41.0 8610.0 8299.0 4610.0\n",
            " 4923.0 8121.0 491.0 61.0 8423.0 4662.0 7310.0 2910.0 4772.0 47.0 2220.0\n",
            " 478.0 2100.0 6311.0 3030.0 5210.0 6920.0 8430.0 1079.0 6110.0 2610.0\n",
            " 8522.0 6201.0 851.0 6430.0 433.0 421.0 3313.0 6202.0 5510.0 2011.0 5610.0\n",
            " 145.0 2651.0 1410.0 651.0 2432.0 8010.0 1.0 2391.0 711.0 4759.0 4753.0\n",
            " 42.0 4690.0 8549.0 4510.0 810.0 7912.0 4663.0 8530.0 731.0 1030.0 5320.0\n",
            " 6492.0 462.0 4652.0 6511.0 4641.0 4321.0 2811.0 4721.0 4764.0 8412.0\n",
            " 1061.0 4752.0 4210.0 1010.0 5820.0 2592.0 692.0 8413.0 331.0 4741.0\n",
            " 4649.0 2710.0 2750.0 5223.0 6499.0 2670.0 4659.0 852.0 6391.0 275.0\n",
            " 4661.0 1811.0 2511.0 4719.0 8211.0 2825.0 4390.0 2013.0 45.0 31.0 4520.0\n",
            " 93.0 1520.0 4771.0 2812.0 4723.0 161.0 4290.0 6512.0 63.0 1623.0 4911.0\n",
            " 561.0 7020.0 2822.0 741.0 6820.0 475.0 9998.0 241.0 2630.0 6209.0 5819.0\n",
            " 6910.0 3290.0 311.0 9601.0 2393.0 5630.0 4761.0 463.0 7710.0 351.0 5224.0\n",
            " 27.0 10.0 2930.0 329.0 611.0 2821.0 6619.0 5120.0 1050.0 8020.0 4912.0\n",
            " 9602.0 8620.0 2591.0 4630.0 8690.0 8790.0 1200.0 474.0 4921.0 4530.0\n",
            " 5110.0 1312.0 2410.0 3314.0 2790.0 7490.0 6622.0 2620.0 65.0 163.0 266.0\n",
            " 28.0 1702.0 4763.0 2219.0 641.0 5812.0 3520.0 1101.0 25.0 471.0 6120.0\n",
            " 6411.0 6612.0 nan 9200.0 7120.0 7410.0 71.0 220.0 9511.0 5012.0 2420.0\n",
            " 1392.0 49.0 2829.0 383.0 6312.0 1020.0 6491.0 5621.0 8542.0 5911.0 86.0\n",
            " 5811.0 6020.0 451.0 4722.0 62.0 30.0 861.0 960.0 2211.0 2817.0 4220.0\n",
            " 5813.0 6810.0 702.0 6621.0 1622.0 620.0 2023.0 7320.0 2819.0 466.0 2824.0\n",
            " 1104.0 9499.0 60.0 68.0 781.0 531.0 201.0 4311.0 4540.0 91.0 3821.0 853.0\n",
            " 2599.0 3600.0 5221.0 990.0 7820.0 7010.0 2593.0 1920.0 4651.0 26.0 2310.0\n",
            " 5310.0 3900.0 6611.0 472.0 1621.0 2392.0 2640.0 3020.0 8110.0 8220.0 46.0\n",
            " 842.0 221.0 8130.0 251.0 7730.0 12.0 5629.0 9000.0 8129.0 6190.0 3250.0\n",
            " 1102.0 2012.0 3830.0 4922.0 8890.0 812.0 7911.0 722.0 4312.0 2513.0\n",
            " 1430.0 23.0 9603.0 43.0 682.0 3315.0 3211.0 803.0 124.0 '86' '6920'\n",
            " '9999' '5221' '8549' '47' '5911' '692' '9996' '8129' '4799' '6120' '4752'\n",
            " '7320' '5610' '8790' '26' '4100' '8413' '8610' '9998' '46' '6411' '0146'\n",
            " '649' '8411' '63' '6430' '7110' '421' '2930' '68' '251' '4711' '2392'\n",
            " '6512' '6419' '4390' '5811' '8510' '852' '2023' '0721' '8521' '731'\n",
            " '8412' '7020' '4669' '16' '4510' '21' '6622' '5819' '5110' '1410' '8690'\n",
            " '851' '4330' '5629' '78' '9200' '4630' '7912' '8710' '3600' '4649' '2790'\n",
            " '6201' '85' '4312' '4771' '3510' '8220' '7310' '3312' '9602' '1101'\n",
            " '8620' '2620' '6202' '711' '6520' '0113' '2910' '6110' '8542' '2220'\n",
            " '1430' '8522' '2593' '2420' '4741' '6619' '4730' '8530' '4322' '282'\n",
            " '9511' '4763' '2599' '2822' '491' '2819' '1312' '2394' '2821' '2100'\n",
            " '8010' 'G' '13' '463' '4321' '4721' '6810' '7210' '8423' '4759' '4923'\n",
            " '4220' '7830' '62' '9312' '2512' '3100' '7010' '3314' '7820' '6621'\n",
            " '3811' '4210' '3030' '2432' '2011' '7220' '7500' '5621' '5510' '4772'\n",
            " '471' '2920' '2812' '7490' '9491' '4659' '2825' '2391' '0210' '1040'\n",
            " '7420' '8422' '4662' '1702' '8110' '6910' '2829' '4719' '6820' '2013'\n",
            " '3020' '4921' '43' '9329' '4773' '5820' '1709' '612' '7410' '2012' '4922'\n",
            " '4610' '5210' '453' '6190' '8730' '6020' '012' '3290' '1811' '41' '4520'\n",
            " '721' '9609' '2813' '2022' '433' '2750' '65' '2610' '9311' '5229' '28'\n",
            " '4290' '2710' '2511' '781' '6612' '7710' '5813' '6511' '4661' '0119'\n",
            " '353' '0161' '5590' '7120' '9411' '1071' '465' '4653' '2211' '1079'\n",
            " '0322' '2410' '8121' '422' '4311' '3700' '8292' '1520' '141' '9103' '451'\n",
            " '10' '0141' '2395' '4722' '27' '3250' '889' '8430' '2591' '351' '4690'\n",
            " '2670' '61' '4663' '4329' '4641' '33' '5120' '5630' '9000' '9499' '2826'\n",
            " '1050' '6492' '4530' '4791' '1073' '6209' '2021' '493' '01' '1102' '1920'\n",
            " '1701' '855' '2630' '2824' '2816' '2219' '7729' '531' '9412' '6499'\n",
            " '0163' '1030' '611' '4742' '0150' '2310' '561' '0990' '60' '8130' '0164'\n",
            " '861' '4753' '49' '4620' '06' '2811' '1010' '56' '5223' '2651' '869'\n",
            " '0810' '0121' '5310' '0910' '0111' '4751' '3092' '93' '9997' '651' '2732'\n",
            " '581' 4669.0 855.0 2395.0 9522.0 9101.0 8292.0 7420.0 1701.0 8730.0\n",
            " 1610.0 111.0 322.0 51.0 202.0 841.0 9997.0 7500.0 119.0 1311.0 105.0 35.0\n",
            " 862.0 551.0 4329.0 473.0 461.0 854.0 9491.0 4799.0 3011.0 5229.0 2399.0\n",
            " 131.0 4653.0 9311.0 9609.0 2431.0 6399.0 869.0 36.0 712.0 2512.0 1080.0\n",
            " 4620.0 1512.0 5912.0 899.0 58.0 9529.0 2826.0 84.0 6.0 210.0 9329.0\n",
            " 8810.0 2813.0 2022.0 3811.0 281.0 146.0 843.0 2021.0 9103.0 889.0 510.0\n",
            " 9312.0 21.0 6420.0 2823.0 410.0 90.0 '4761' '8890' '8020' '741' '2680'\n",
            " '1103' '3520' '502' '84' '2660' '324' '1061' '2592' '3211' '131' '6312'\n",
            " '8421' '900' '1080' '853' '1621' '90' '1629' '4651' '6311' '4762' '64'\n",
            " '8230' '2396' '432' '802' '478' '5224' '473' '1104' '383' '620' '4764'\n",
            " '0510' '6130' '3830' '0124' '275' '6491' '2431' '4774' '0893' '1610' '71'\n",
            " '3315' '108' '8541' '812' '841' '24' '25' '3011' '2733' '8211' '511'\n",
            " '9420' '42' '8550' '2520' '871' '2652' '8291' '2823' '702' '271' '8299'\n",
            " '1812' '2640' '1623' '1622' '0311' '222' '5012' '464' '2029' '9102'\n",
            " '5011' '2740' '20' '266' '1200' '0620' '329' '0149' '281' '263' '110'\n",
            " '0123' '192' '45' '1392' '0729' '5912' '201' '55' '4912' '9319' '439'\n",
            " '5320' '4911' '7911' '9101' '477' '3320' '771' '5222' '4652' '6530' '82'\n",
            " '0899' '682' '6630' '7730' '210' '293' '6629' '8219' '862' '4540' 9321\n",
            " 282 581 8230 3312 6630 7220 4791 64 6629 141 4782 13 103 1812 9521 7830\n",
            " 9412 5914 871 891 8710 941 1073 7810 9411 2720 1709 9319 2816 2733 87 432\n",
            " 5913 1313 492 6530 325 5011 2030 164 453 7990 5222 612 72 261 2680 900\n",
            " 1103 '8810' '105' '3900' '811' '2720' '231' '9601' '202' '742' '11' '30'\n",
            " '842' '77' '681' '653' '492' '07' '3530' '3822' '854' '17' '466' '2513'\n",
            " '161' '9522' '3012' 'C' '7810' '5812' 1074.0 2394.0 3092.0 881.0 6130.0\n",
            " 3099.0 125.0 110.0 16.0 1399.0 2732.0 1075.0 452.0 279.0 910.0 75.0\n",
            " 4742.0 9700.0 123.0 78.0 8550.0 252.0 9524.0 92.0 9420.0 '2399' '92'\n",
            " '0891' '1020' '87' '88' '36' '4781' '472' '773' '74' '0144' '6391' '475'\n",
            " '1311' '259' '09' '15' '474' '31' '0610' '1399' 3320.0 1072.0 7.0 2652.0\n",
            " 11.0 3822.0 879.0 8291.0 17.0 3530.0 3700.0 56.0 477.0 502.0 77.0 74.0]\n",
            "Number of unique values: 946\n",
            "\n",
            "######### ctryqual ##########\n",
            "[nan 'North America and Western Europe' 'Central and Eastern Europe'\n",
            " 'East Asia and the Pacific (richer countries)'\n",
            " 'East Asia and the Pacific (poorer countries)' 'Sub-Saharan Africa'\n",
            " 'Latin America and the Caribbean' 'Arab States' 'South and West Asia'\n",
            " 'Central Asia']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### ageg5lfs ##########\n",
            "['Aged 25-29' 'Aged 60-65' 'Aged 30-34' 'Aged 20-24' 'Aged 35-39'\n",
            " 'Aged 40-44' 'Aged 45-49' 'Aged 55-59' 'Aged 50-54' 'Aged 16-19']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### v92 ##########\n",
            "[nan 'Full-time employed (self-employed, employee)' 'Unemployed'\n",
            " 'Part-time employed (self-employed, employee)'\n",
            " 'Fulfilling domestic tasks or looking after children/family'\n",
            " 'Pupil, student' 'Apprentice, internship' 'Permanently disabled' 'Other'\n",
            " 'In retirement or early retirement'\n",
            " 'In compulsory military or community service']\n",
            "Number of unique values: 11\n",
            "\n",
            "######### lng_home ##########\n",
            "['999' 'eng' 'rus' 'kor' 'pol' 'fra' 'jpn' 'lit' 'nld' 'spa' 'slv' 'dan'\n",
            " 'slk' 'heb' 'ces' 'zho' 'est' 'ell' 'fin' 'deu' 'nor' 'ita' 'cat' 'ara'\n",
            " 'glg' 'guj' 'hye' 'eus' 'por' 'hun' 'hrv' 'swe' 'tam' nan 'fas' 'kur'\n",
            " 'ber' 'ben' 'srp' 'tur' 'hin' 'ron' 'sin' 'urd' 'run' 'phi' 'gla' 'bul'\n",
            " 'sla' 'pap']\n",
            "Number of unique values: 50\n",
            "\n",
            "######### v31 ##########\n",
            "['General programmes' 'Engineering, manufacturing and construction'\n",
            " 'Social sciences, business and law' nan\n",
            " 'Science, mathematics and computing' 'Agriculture and veterinary'\n",
            " 'Health and welfare' 'Humanities, languages and arts' 'Services'\n",
            " 'Teacher training and education science']\n",
            "Number of unique values: 10\n",
            "\n",
            "######### cntryid ##########\n",
            "['Canada' 'United Kingdom' 'Norway' 'United States' 'Russian Federation'\n",
            " 'Turkey' 'Sweden' 'Germany' 'Korea' 'Ireland' 'Singapore' 'Italy'\n",
            " 'Poland' 'France' 'Japan' 'Netherlands' 'Chile' 'Spain' 'Slovenia'\n",
            " 'Denmark' 'Belgium' 'Slovak Republic' 'Israel' 'Czech Republic' 'Estonia'\n",
            " 'Greece' 'Finland' 'Austria' 'New Zealand' 'Lithuania']\n",
            "Number of unique values: 30\n",
            "\n",
            "######### cnt_brth ##########\n",
            "[nan 'United Kingdom of Great Britain and Northern Ireland'\n",
            " 'Russian Federation' 'Turkey' 'Republic of Korea' 'Malaysia' 'Poland'\n",
            " 'France' 'Japan' 'Lithuania' 'Netherlands' 'Chile' 'Ethiopia' 'Spain'\n",
            " 'Croatia' 'Denmark' 'Belgium' 'Slovakia' 'Israel' 'Czech Republic'\n",
            " 'Venezuela (Bolivarian Republic of)' 'Canada' 'Estonia' 'Greece'\n",
            " 'Finland' 'South Africa' 'Austria' 'Republic of Moldova' 'Algeria'\n",
            " 'Italy' 'Morocco' 'Singapore' 'China' 'Australia' 'Colombia' 'Ghana'\n",
            " 'Argentina' 'India' 'Slovenia' 'Armenia' 'Dominican Republic' 'Zimbabwe'\n",
            " 'Tunisia' 'Germany' 'Ukraine' 'Cuba' 'Brazil' 'Uzbekistan' 'Switzerland'\n",
            " 'Senegal' 'Philippines' 'Egypt'\n",
            " 'The former Yugoslav Republic of Macedonia' 'Bosnia and Herzegovina'\n",
            " 'United States of America' 'Peru' 'Thailand' 'Georgia' 'Congo' 'Romania'\n",
            " 'Afghanistan' 'Mexico' 'Saint Lucia' 'Bangladesh' 'Kenya' 'Bolivia'\n",
            " 'Paraguay' 'Indonesia' 'Serbia' 'Cyprus' 'Portugal' 'Ireland' 'Sri Lanka'\n",
            " 'Myanmar' 'Belarus' 'Burundi' 'Cameroon' 'Azerbaijan' 'Mauritius' 'Iraq'\n",
            " 'Albania' 'Viet Nam' 'Sweden' 'Latvia'\n",
            " 'Hong Kong Special Administrative Region of China' 'Norway'\n",
            " 'Trinidad and Tobago' 'Nigeria' 'Kyrgyzstan']\n",
            "Number of unique values: 89\n",
            "\n",
            "######### isic2l ##########\n",
            "[nan 9996.0 21.0 55.0 85.0 66.0 9995.0 10.0 47.0 94.0 86.0 43.0 64.0 41.0\n",
            " 68.0 14.0 31.0 84.0 '9996' '10' '47' '88' '9995' '85' '56' '43' '14' '22'\n",
            " '84' '9999' '62' '28' '68' '21' '86' '64' '61' '31' '29' '66' '69' 9999.0\n",
            " 69.0 46.0 53.0 50.0 '50' '52' '46' '23' '41' '65' '87' '49' '72' '16'\n",
            " 61.0 22.0]\n",
            "Number of unique values: 58\n",
            "\n",
            "There are 24 categorical columns with more than 10 categories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjjAdbrm6j6l",
        "colab_type": "text"
      },
      "source": [
        "## Impressions on missing categorical data\n",
        "Proportionally, there are more missing numerical feature values than there are categorical feature values. There are some categorical feature values that use numeric values to indicate a discrete category or range. This can be gleened by particular values such as '9999' or '999' to indicate a missing value or distinct category. Some of the features that encode their values as such include:\n",
        "* ~~reg_tl2~~\n",
        "* ~~lng_home~~\n",
        "* ~~ageg10lfs~~\n",
        "* ~~ageg10lfs_t~~\n",
        "\n",
        "These features are seemingly identical, and therefore we select one to keep and we drop the other:\n",
        "* ~~ageg10lfs~~\n",
        "* ~~ageg10lfs_t~~\n",
        "\n",
        "One feature has 946 unique values which probably indicates that the values maybe numeric in nature as opposed to categorical. There are also two alphabetical letter values feature, namely : 'C' and 'G'. These values will be imputed as np.nan values before the the numeric casting of the feature column. Additionally, the numeric values in this feature are actually strings and will have to be forced cast to int or float values\n",
        "* ~~v71~~\n",
        "\n",
        "Another categorical feature uses extensive numerical coding, interleaved with few alphabetic numeric coding. The instict is to cast the alphabetical coding can be cast to np.nan before the entire feature category can be cast as a numeric column, however, after looking at the value_counts for each of categories in this feature, we can decide to leave it alone as there are a signficant numer of data points that use the alphabetical encoding:\n",
        "* ~~isic2c~~\n",
        "\n",
        "Can be cast to numeric:\n",
        "* ~~isic2l~~\n",
        "\n",
        "This feature seems to be a more granular subfeature of 'cntryrgn' and a few other of the country based features, however this feature has significantly more unique values (176 unique values). As it is a sparser representation of the other superfeatures, it can be dropped:\n",
        "* ~~reg_tl2~~\n",
        "\n",
        "There are a lot of missing values for the lng_home feature when we look at the value_counts. If questionnaire was completed in English, and the language of the exercise was done in English, then may be safe to assume that the primary language spoken at home maybe the same language. Therefore, we will apply this logic for all the missing values in the lng_home feature. These are the language categorical variables.\n",
        "* ~~lng_bq~~\n",
        "* ~~lng_ci~~\n",
        "* ~~lng_home~~\n",
        "\n",
        "Some features have exactly only one categorical value. These can be dropped as they do no add further information\n",
        "* ~~uni -> only value is 'cl3770'~~\n",
        "\n",
        "Instead of 'No' as the other binary value in a categorical feature, this feature has NaN instead... This value will have to be replaced with the string 'No':\n",
        "* ~~v270~~\n",
        "\n",
        "Some features have a large category space, which would balloon the dataset once onehot-encoded. There are 24 columns with more than 10 categories and they are: \n",
        "* ['isic1l', 'ageg5lfs', 'v71', 'earnmthalldcl', 'earnhrdcl', 'ctryqual', 'cntryid', 'reg_tl2', 'cnt_brth', 'isic1c', 'lng_ci', 'cntryid_e', 'v92', 'v59', 'edcat8', 'v31', 'lng_bq', 'v19', 'isic2l', 'lng_home', 'birthrgn', 'v212', 'earnhrbonusdcl', 'isic2c']\n",
        "\n",
        "There are also many seemingly highly correlated and repeated features that can be dropped which pertain to information about which country or region an employee is from:\n",
        "* 'ctryrgn', 'ctryqual', 'birthrgn', 'cntryid_e', 'cntryid'\n",
        "\n",
        "Our best bet would be to select to keep just one of these features - preferrably a feature with a relatively low number of categories, and then drop the rest.\n",
        "\n",
        "Therefore we will try to do our best to reduce the number of categories by possibly coming up with higher level categories to encompass these features or cast them as numeric features if the majority of their values are numerically encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbP4o1MWdok5",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Imputer Imputation for categorical features\n",
        "Therefore, some of the imputation and preprocessing can be done before the encoding step of the preprocessing portion of the pipeline (dropping,casting, merging, etc). The next few segments of code will go through the next preprocessing steps as outlined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmrAYnBWVYco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc381958-0ce3-4794-fde1-0f112bb9af71"
      },
      "source": [
        "# NaN's -> 'No' for 'v270'\n",
        "\n",
        "values = {'v270': 'No'}\n",
        "categorical_df.fillna(value=values, inplace=True)\n",
        "print(categorical_df['v270'].unique())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Yes' 'No']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWjzCLPDFXc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# address missing values for lng_home  and the other language features\n",
        "\n",
        "categorical_df['lng_home'] = categorical_df['lng_bq'].where(categorical_df['lng_bq'] == categorical_df['lng_ci'])\n",
        "\n",
        "# language features\n",
        "import re\n",
        "language = ['lng_bq', 'lng_ci', 'lng_home']\n",
        "num_pattern = re.compile('\\d\\d\\d')\n",
        "\n",
        "# replace the 999 value with np.nan\n",
        "for col in language:\n",
        "    categorical_df[col].replace(to_replace=num_pattern, value=np.nan, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly3EHoPutlhU",
        "colab_type": "code",
        "outputId": "e1be672d-e748-4a2b-e297-cb1d4614669f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# categorical features with only one value\n",
        "\n",
        "for column in categorical_df.columns:\n",
        "    if len(categorical_df[column].unique()) <=1:\n",
        "        print(column)\n",
        "        print(\"The only unique value for {} is: {}\".format(column, categorical_df[column].unique()))\n",
        "\n",
        "# dropping column 'uni' with value only cl3770\n",
        "categorical_df.drop(columns='uni', inplace=True, axis=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uni\n",
            "The only unique value for uni is: ['cl3770']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTC2aJ9VwLpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cast 'isic2l' to numeric\n",
        "\n",
        "categorical_df['isic2l'] = categorical_df['isic2l'].fillna(-1)\n",
        "categorical_df['isic2l'] = categorical_df['isic2l'].astype(int)\n",
        "categorical_df['isic2l'] = categorical_df['isic2l'].astype(str)\n",
        "categorical_df['isic2l'] = categorical_df['isic2l'].replace('-1', np.nan)\n",
        "numeric_df['isic2l'] = categorical_df['isic2l'].values.astype(np.float64)\n",
        "categorical_df.drop(columns='isic2l', axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRL3vQQ0zGen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use regex to replace the letter values with np.nan values for feature v71\n",
        "import re\n",
        "pattern = re.compile(\"[a-zA-Z]\")\n",
        "\n",
        "categorical_df['v71'].replace(to_replace=pattern, value=np.nan, inplace=True)\n",
        "\n",
        "# then we cast the stringed numeric values in this column to int or float\n",
        "numeric_df['v71'] = categorical_df['v71'].astype(np.float64)\n",
        "categorical_df.drop(columns='v71', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3tGFRn35ETy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# addressing the missing values of feature reg_tl2\n",
        "\n",
        "categorical_df.drop(columns='reg_tl2', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3wrlIP8DQzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from here, we select only one of these to keep and we will drop the rest\n",
        "\n",
        "country_features = ['ctryrgn', 'ctryqual', 'birthrgn', 'cntryid_e', 'cntryid']\n",
        "# categorical_df[country_features].head()\n",
        "\n",
        "# we decide to only keep the ctryrgn and drop the rest for now, however, cntryid is another candidate to keep if we wanted more granularity with respect to country information\n",
        "categorical_df.drop(columns=['ctryqual', 'birthrgn', 'cntryid_e', 'cntryid'], inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq7HttdTEh3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we drop 'ageg10lfs_t' because it contains identical values to 'ageg10lfs'\n",
        "\n",
        "categorical_df.drop(columns='ageg10lfs_t', inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf3eCPwUqDKM",
        "colab_type": "text"
      },
      "source": [
        "# Encoding the Categorical Variables\n",
        "\n",
        "We pick an encoding scheme to create dummy encoded categorical variables from our large array of categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yqQEDWtygmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we choose to choose a simple one-hot encoding scheme for our features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder()\n",
        "enc.fit(np.array(drop_null_missing_df))\n",
        "encoded_df = enc.transform(drop_null_missing_df)\n",
        "print(encoded_df(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmwWHJJKIPKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frequency numeric distribution of features \n",
        "feature_labels = [i for i in drop_null_missing_df.columns if i != \"job_performance\"]\n",
        "\n",
        "for i in feature_labels[:5]:\n",
        "    print(\"###### {} ######\\n\".format(i))\n",
        "    print(drop_null_missing_df[i].value_counts())\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCFvnIGg4aUp",
        "colab_type": "text"
      },
      "source": [
        "## Imputation for Missing Values\n",
        "We want to then use an informed approach to imputation of missing data values. One method could be the use of KNN imputation or random forest and proximity matrix to imputing the missing values for our dataset with some similarity metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GHjiOiAh8J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install missingpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM23FWFvlWJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# missing value imputation with Random Forest\n",
        "# from missingpy import MissForest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSEwyHGiiBFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# missing value imputation with KNN\n",
        "from missingpy import KNNImputer\n",
        "\n",
        "knn = KNNImputer(missing_values=\"NaN\", n_neighbors=3, weights=\"uniform\",\n",
        "                 metric=\"masked_euclidean\", row_max_missing=0.5,\n",
        "                 col_max_missing=0.8, copy=True)\n",
        "\n",
        "knn_missing_imputation = knn.fit_transform(final_kept_df)\n",
        "\n",
        "print(type(knn_missing_imputation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCen5qo8XrY3",
        "colab_type": "text"
      },
      "source": [
        "## Scaling and Normalization\n",
        "\n",
        "This is a step we may or may not need to undertake based on the regressor that we choose to model with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um24MZGnJjqm",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection: Forward Selection\n",
        "\n",
        "We enter our feature selection step of the pipeline and we use an out of the box forward feature selection method from ML XTend to pick the features that result in the best predictive ability of our models. There are a number of different feature selection techniques that we could use (and we may explore that in the future), but for now, using a simple forward selection process is what we are going to stick with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX1-22kQqcyB",
        "colab_type": "text"
      },
      "source": [
        "# Dev Branch 1 - Data Visualization for EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dPcM7UuxkWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we plot the several categorical variables of interest against the job performance metric to visualize the distribution of job performance among the categories\n",
        "# Make the plots a little bigger\n",
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "feature_labels = [i for i in drop_null_missing_df.columns if i != \"job_performance\"]\n",
        "\n",
        "# create a FacetGrib object, on which to map your dataframe data onto\n",
        "g = sns.FacetGrid(drop_null_missing_df, col=\"cntryid\")\n",
        "\n",
        "# Using the FacetGrid.map() function, we will be able to visualize our data. \n",
        "# Provide the map() function with a mapping function and the name of the numeric class variable in your dataframe to plot against\n",
        "# each of your categorical variables\n",
        "g.map(plt.hist, \"job_performance\", alpha=0.7)\n",
        "g.add_legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvfFNv6So1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution plots per categorical variable. Using A facetGrid object with the sns.distplot mapping\n",
        "# function works a little differently so you will have to specify a things like row_order\n",
        "categories_list = drop_null_missing_df.cntryid.value_counts().index\n",
        "\n",
        "g = sns.FacetGrid(drop_null_missing_df, row=\"cntryid\", row_order=categories_list,\n",
        "                  height=2.0, aspect=3,)\n",
        "g.map(sns.distplot, \"job_performance\", hist=True, rug=True);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4VMMccVCdHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# groupby country id\n",
        "grouped = drop_null_missing_df.groupby(\"cntryid\").groups\n",
        "print(grouped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CO8samBPrJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the facetgrid for every feature against the job_performance metric\n",
        "for feature in feature_labels:\n",
        "    cat_list = drop_null_missing_df[feature].value_counts().index\n",
        "    g = sns.FacetGrid(drop_null_missing_df, row=feature, row_order=cat_list,\n",
        "                      height=1.7, aspect=4)\n",
        "    g.map(sns.distplot, \"job_performance\", hist=True, rug=True);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_sjwvIKpwbn",
        "colab_type": "text"
      },
      "source": [
        "# Dev Branch 1 - Handling Missing Values - Imputation Step\n",
        "\n",
        "When we pick an imputation strategy for the our data, we have to consider our data is numeric or non-numeric. We also have to consider whether the variable is nominally categorical or ordinally categorical to determine what type of transformative encoding scheme we can apply to those features.\n",
        "\n",
        "We also have to consider that the implications that one-hot encoding would have on our tabular dataset when we already have so many features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmKCm-kGL1w",
        "colab_type": "text"
      },
      "source": [
        "## Imputation with a Strategy - EDA\n",
        "\n",
        "Before we even start imputng, we would want to visually understand the distribution and nature of our missing data.  For example, if the data has seasonality, it would make little sense to impute with a mean value, however - if the data are constant, it may a good idea to impute the missing data value with the mean or median value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYJag0GyFEhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the missing data features\n",
        "\n",
        "missing_val_features = [feature for feature in df if df[feature].isnull().sum() != 0]\n",
        "\n",
        "print(\"There are {} features with missing values\".format(len(missing_val_features)))\n",
        "\n",
        "for i in missing_val_features:\n",
        "    print(\"### {} ###\".format(i))\n",
        "    print(df[i].value_counts())\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO-VRi-hUgab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# each element in df.dtypes is the datatype of that feature\n",
        "print(df[missing_val_features].dtypes[:10]) # i is the string of datatypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96-NzVzrO_Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# have a look at the distribution of the missing values in the features that were dropped containing\n",
        "# missing values\n",
        "\n",
        "# for feature in missing_val_features:\n",
        "#     if df[feature].dtype == \"object\":\n",
        "#         fig = plt.figure(1, figsize=(8, 14), frameon=False, dpi=100)\n",
        "#         g = sns.FacetGrid(df, col=feature)\n",
        "#         g.map(plt.hist, \"job_performance\", alpha=0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKKHKBn3vo6F",
        "colab_type": "text"
      },
      "source": [
        "## Univariate Crosstab Analysis\n",
        "\n",
        "From what we can see, a portion of our data are categorical variables and it will be important to visually explore and visualize using cross tabulation, which is available from the pandas API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLK_6SQQyArr",
        "colab_type": "text"
      },
      "source": [
        "### Cross Tab Frequency Plots of Features that were dropped due to missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0uHYdP5vlcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # create a cross tabulation for each categorical feature and then plot it\n",
        "# # These are frequency plots of each of the values that are categorical\n",
        "# # these plots include the features that were dro\n",
        "\n",
        "# for feature in missing_val_features:\n",
        "#     if df[feature].dtype == \"object\":\n",
        "#         tabulation = pd.crosstab(index=df[feature], columns=\"count\")\n",
        "#         tabulation.plot.bar(color=\"green\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol9ldL7ZyHVM",
        "colab_type": "text"
      },
      "source": [
        "### Cross tab frequency plots of categorical features that did not contain any missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSLNED39yG9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for feature in feature_labels:\n",
        "#     if drop_null_missing_df[feature].dtype == \"object\":\n",
        "#         tabulation = pd.crosstab(index=drop_null_missing_df[feature], columns=\"count\")\n",
        "#         tabulation.plot.bar(color=\"purple\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5bVhtyp1bOW",
        "colab_type": "text"
      },
      "source": [
        "## Bivarate Analysis using Box Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBN6aCyO1kWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.set(style=\"whitegrid\")\n",
        "\n",
        "# for feature in feature_labels:\n",
        "#     if drop_null_missing_df[feature].dtype == \"object\":\n",
        "#         g = sns.catplot(x=feature, \n",
        "#                     y=\"job_performance\", \n",
        "#                     data=drop_null_missing_df,\n",
        "#                     palette=\"Set2\",\n",
        "#                    kind=\"box\")\n",
        "#         g.set_xticklabels(rotation=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buUHOd-mmcBO",
        "colab_type": "text"
      },
      "source": [
        "# Dev Branch 1 - Pre-Processing: Encoding Categorical Variables\n",
        "\n",
        "For simplicity sake, we go with a one hot encoding scheme for our variables, knowing full well that this will lead to a very sparse datset. We will try and not think too hard about this at this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0-9CVkcnFQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_df = drop_null_missing_df.loc[:, drop_null_missing_df.columns != 'job_performance'] # print(pd.get_dummies(data))\n",
        "\n",
        "# Separate out the features from the class\n",
        "dummy_features = pd.get_dummies(drop_null_missing_df.loc[:, drop_null_missing_df.columns != 'job_performance'])\n",
        "X = dummy_features.values\n",
        "y = drop_null_missing_df.loc[:, \"job_performance\"].values\n",
        "\n",
        "dummy_features = pd.get_dummies(drop_null_missing_df.loc[:, drop_null_missing_df.columns != 'job_performance'])\n",
        "dummy_features[\"job_performance\"] = y\n",
        "\n",
        "print(\"Shape of features dataframe before dummification: {}\".format(features_df.shape))\n",
        "print(\"Shape of features dataframe after dummification: {}\".format(dummy_features.shape))\n",
        "print(\"The dataframe has increase {0:.1f}x in feature size after dummification\".format(dummy_features.shape[1]/features_df.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_7DeDfdx6lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we also need to reset the index\n",
        "dummy_features = dummy_features.reset_index()\n",
        "dummy_features = dummy_features.iloc[:, 2:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5sUalIx6kBx",
        "colab_type": "text"
      },
      "source": [
        "Indeed, we are working with a much more sparse dataset after we have assigned dummy variables to the categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwcHh42w6rYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Due to the sheer volume of encodings for these listed columns, we will keep these as typed\n",
        "# as integers as opposed to the nominal nature of the their encoding\n",
        "\n",
        "iscolc_columns = [\"isco1c\", \"isco2c\", \"isco1l\", \"isco2l\"]\n",
        "for col in iscolc_columns:\n",
        "    print(f\"#### {i} ###\")\n",
        "    print(drop_null_missing_df[col].value_counts()) #remember, value_counts() is a method\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ThimKJveRr3",
        "colab_type": "text"
      },
      "source": [
        " Multiple Correspondence Analysis with the mca package\n",
        " \n",
        " tetrachoric correlation matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVWlxUBCp4CH",
        "colab_type": "text"
      },
      "source": [
        "# Separate the Features from the Class Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvaQzvVdwFf7",
        "colab_type": "text"
      },
      "source": [
        "## Subsetting the dataset\n",
        "We are randomly sampling the dataset for  subset that data so that we can  train much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgM1ZnKNwEgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subdf = df.sample(n=int(0.6*df.shape[0]), random_state=123)\n",
        "subdf.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWi977G3nTQs",
        "colab_type": "text"
      },
      "source": [
        "## Non-scaled data partitioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QQul_MRo-Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H6or-lC9uM2",
        "colab_type": "text"
      },
      "source": [
        "## Standardizing and Normalizing our data\n",
        "\n",
        "Some of the models we use are sensitive to scaling and perform better with it. Some do not require it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um-sYGcA9thZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "scaled_X = scaler.fit_transform(X)\n",
        "scaled_y = scaler.fit_transform(y.reshape(-1,1))\n",
        "\n",
        "# split into training and testing sets\n",
        "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
        "                                                    scaled_X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.30,\n",
        "                                                    random_state=42\n",
        "                                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kofMoVAHcmPm",
        "colab_type": "text"
      },
      "source": [
        "# Dev Branch 1a - Create Null Models\n",
        "\n",
        "In this block, we will create our null models to build upon. We will be using SVR, Linear Regression, MLPs and simple neural networks to start off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3GuRO7GLst-",
        "colab_type": "text"
      },
      "source": [
        "## Dev Branch 1a -  Simple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyf23iwzclap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple Linear Model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# normalization of your features is not advised here because it makes it hard to interpret \n",
        "# the coefficients and they don't normalize well.\n",
        "\n",
        "lm = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=None)\n",
        "lm.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Model Score: {}\\n\".format(lm.score(X_train, y_train)))\n",
        "print(\"Linear Model Coefficient:\\n {}\\n\".format(lm.coef_[:10]))\n",
        "print(\"Linear Model Intercept: {}\\n\".format(lm.intercept_ ))\n",
        "y_pred = lm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7iZFhFgFiqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inverse_y_pred = scaler.inverse_transform(y_pred)\n",
        "print(\"Inverse transform of mse pred\\n{}\\n\".format(inverse_y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m47JngTwrH3h",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "Surprisingly, a high dimensional linear model predicting job performance scores, with default parameters,  returns a relatively high coefficient of determination R2 of 0.33490222049791485, wrt to the number of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzd0EbhRPZ0e",
        "colab_type": "text"
      },
      "source": [
        "## Dev Branch 1a - Support Vector Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBiFcxr_PX1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Support Vector Machine for Regression\n",
        "from sklearn.svm import SVR\n",
        "import timeit\n",
        "\n",
        "svr = SVR(kernel=\"rbf\", degree=3, gamma=\"auto_deprecated\", coef0=0.0, tol=0.001, \n",
        "          C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, \n",
        "          max_iter=-1)\n",
        "\n",
        "# Using the normalized inputs\n",
        "start_time = timeit.default_timer()\n",
        "y_pred = svr.fit(X_train_scaled, y_train)\n",
        "end_time = timeit.default_timer()\n",
        "print(\"Run time: {}\".format(end_time - start_time))\n",
        "\n",
        "print(\"SVR Model Score: {}\\n\".format(svr.score(X_train_scaled, y_train)))\n",
        "print(\"SVR Model Params:\\n {}\\n\".format(svr.get_params))\n",
        "y_pred = svr.predict(X_test_scaled)\n",
        "print(\"y_pred: \\n{}\".format(y_pred))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYLGKWRBqfLQ",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "As we can expect from this high dimensional data modelling, we get a terrible R^2 coefficient of determination of the prediction error of 0.09891703260752771"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDqlyEnnYaje",
        "colab_type": "text"
      },
      "source": [
        "## Dev Branch 1a - Support Vector Regression with Different kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-Ay9CeYXbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
        "svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
        "svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,\n",
        "               coef0=1)\n",
        "\n",
        "lw = 2\n",
        "\n",
        "svrs = [svr_rbf, svr_lin, svr_poly]\n",
        "kernel_label = ['RBF', 'Linear', 'Polynomial']\n",
        "model_color = ['m', 'c', 'g']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQVc93oEvdNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_scaled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_XEagzYlSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the results\n",
        "# ig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\n",
        "# for ix, svr in enumerate(svrs):\n",
        "#     axes[ix].plot(X_train_scaled, svr.fit(X_train_scaled, y_train_scaled).predict(X_test_scaled), color=model_color[ix], lw=lw,\n",
        "#                   label='{} model'.format(kernel_label[ix]))\n",
        "#     axes[ix].scatter(X_train_scaled[svr.support_], \n",
        "#                      y_train_scaled[svr.support_], \n",
        "#                      facecolor=\"none\", \n",
        "#                      edgecolor=model_color[ix], \n",
        "#                      s=50, \n",
        "#                      label='{} support vectors'.format(kernel_label[ix]))\n",
        "#     axes[ix].scatter(X_train_scaled[np.setdiff1d(np.arange(len(X_train_scaled)), svr.support_)], \n",
        "#                      y_train_scaled[np.setdiff1d(np.arange(len(X_train_scaled)), svr.support_)],\n",
        "#                      facecolor=\"none\", \n",
        "#                      edgecolor=\"k\", s=50,\n",
        "#                      label='other training data')\n",
        "#     axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),\n",
        "#                     ncol=1, fancybox=True, shadow=True)\n",
        "\n",
        "# fig.text(0.5, 0.04, 'data', ha='center', va='center')\n",
        "# fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')\n",
        "# fig.suptitle(\"Support Vector Regression\", fontsize=14)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uovtpBXaxkgm"
      },
      "source": [
        "# 1b - Using Feature Extraction and Factor Analysis\n",
        "\n",
        "We run multiple null models, after having reduced the number of dimensions and exploring the components of maximum variance from our data.\n",
        "\n",
        "Assumptions for Factor Analysis\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "1. There are no outliers in data.\n",
        "2. Sample size should be greater than the factor.\n",
        "3. There should not be perfect multicollinearity.\n",
        "4. There should not be homoscedasticity between the variables.\n",
        "\n",
        "\n",
        "Types of Factor Analysis\n",
        "\n",
        "* Exploratory Factor Analysis: It is the most popular factor analysis approach among social and management researchers. Its basic assumption is that any observed variable is directly associated with any factor.\n",
        "\n",
        "* Confirmatory Factor Analysis (CFA): Its basic assumption is that each factor is associated with a particular set of observed variables. CFA confirms what is expected on the basic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LQm04n3Kxkgu"
      },
      "source": [
        "## Dev Branch 1b - Simple Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LZsUYhkdxkgx",
        "colab": {}
      },
      "source": [
        "# Simple Linear Model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# normalization of your features is not advised here because it makes it hard to interpret \n",
        "# the coefficients and they don't normalize well.\n",
        "\n",
        "lm = LinearRegression(fit_intercept=True, normalize=True, copy_X=True, n_jobs=None)\n",
        "lm.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Model Score: {}\\n\".format(lm.score(X_train, y_train)))\n",
        "print(\"Linear Model Coefficient:\\n {}\\n\".format(lm.coef_))\n",
        "print(\"Linear Model Intercept: {}\\n\".format(lm.intercept_ ))\n",
        "y_pred = lm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GJUmAHCBxkg_"
      },
      "source": [
        "## Plot SLR model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "95QecFkUxkhB",
        "colab": {}
      },
      "source": [
        "# plt.scatter(X_train, y_train)\n",
        "# plt.plot(np.sort(X_test, axis=0),y_pred)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J85swW5txkhH"
      },
      "source": [
        "## Dev Branch 1b -  Support Vector Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xM-HcU6hxkhI",
        "colab": {}
      },
      "source": [
        "# Support Vector Machine for Regression\n",
        "from sklearn.svm import SVR\n",
        "import timeit\n",
        "\n",
        "svr = SVR(kernel=\"rbf\", degree=3, gamma=\"auto_deprecated\", coef0=0.0, tol=0.001, \n",
        "          C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, \n",
        "          max_iter=-1)\n",
        "\n",
        "# X_train_scaled\n",
        "start_time = timeit.default_timer()\n",
        "y_pred = svr.fit(X_train, y_train)\n",
        "end_time = timeit.default_timer()\n",
        "print(\"Run time: {}\".format(end_time - start_time))\n",
        "\n",
        "print(\"SVR Model Score: {}\\n\".format(svr.score(X_train, y_train)))\n",
        "print(\"SVR Model Params:\\n {}\\n\".format(svr.get_params))\n",
        "y_pred = svr.predict(X_test)\n",
        "print(\"y_pred: \\n{}\".format(y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V8Uc1rc6xkhR"
      },
      "source": [
        "## Plot SVR model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dmq7b6t6xkhS",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZf-3Z2RlZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tpot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1G9C7IiCxkhY"
      },
      "source": [
        "## Dev Branch 1b -TPOT Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdLlrOs5V0TN",
        "colab_type": "text"
      },
      "source": [
        "Teapot does hyperparameter tuning, model selection and preprocessing all in one pipeline. It takes a while to train the say the least. We will select the best parameter from training on the null dataset (without any feature engineering) to see if we can get a functioning model from this exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjhIijPNXEnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        ")\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, n_jobs=-1)\n",
        "\n",
        "prec_rec_fsc_sup = [\"precision\", \"recall\", \"fscore\", \"support\"]\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "tpot.fit(X_train, y_train)\n",
        "y_pred = tpot.predict(X_test)\n",
        "end_time = timeit.default_timer()\n",
        "runtime = end_time - start_time\n",
        "print(f\"Total runtime for the {name} dataset: {runtime}s\")\n",
        "\n",
        "\n",
        "print(\"\\nConfusion Matrix for the {} dataset\\n{}\\n\".format(confusion_matrix(name, y_test, y_pred)))\n",
        "\n",
        "print(\"Precision/Recall/FScore/Support for the {} dataset\".format(name))\n",
        "for met, val in zip(prec_rec_fsc_sup, precision_recall_fscore_support(y_test, y_pred)):\n",
        "    pprint(\"{}: {}\".format(met, val))\n",
        "\n",
        "print(\"Accuracy score for the {} dataset: {}\".format(name, accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTfWeQOiXSSB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Best Performing Models\n",
        "1. LinearSVC\n",
        "Best pipeline: LinearSVC(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), C=20.0, dual=False, loss=squared_hinge, penalty=l2, tol=0.1)\n",
        "Total runtime: 45.1908969899996s\n",
        "Average Accuracy Score: 0.9666666666666667\n",
        "Best Accuracy Score: 0.9825757575757577\n",
        "\n",
        "2. \n",
        "\n",
        "# Wine Dataset\n",
        "1. GradientBoostingClassifier\n",
        "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.5, max_depth=3, max_features=0.1, min_samples_leaf=1, min_samples_split=4, n_estimators=100, subsample=0.6500000000000001)\n",
        "Total runtime: 63.685036884999136s\n",
        "Average Accuracy Score: 0.9722222222222222\n",
        "Best Accuracy Score: 0.9928571428571429\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQjp7HwPRlqa",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "After a while, you start getting an intuition of which hyperparameters are more important than others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9-TvxlyRgNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid search, beam search, bayesian optimization, random search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_lhLXO3ODHS",
        "colab_type": "text"
      },
      "source": [
        "The most successful people are the most effective communicators of results and your visualizations, and why your work matters and what you found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ45fXy-SkH7",
        "colab_type": "text"
      },
      "source": [
        "# Things to do for next week\n",
        "\n",
        "combine all the models into a single classifier\n",
        "weighted stacking of models\n",
        "you can learn this weighting through meta weighting\n",
        "\n",
        "boosting is usually based on weak sequential classifiers\n",
        "this deals with hard edge cases with weak classifiers\n",
        "\n",
        "learn about about whether your model is over fitting\n",
        "to plot, save the RMSE, for each model, and then plot them on a line chart\n",
        "per epoch to see how the different models faired in terms of the number of \n",
        "accuracy vs epochs\n",
        "\n",
        "Also useful for plotting the effects of differnt settings fo different hyperparameters on performance using parallel coordinates\n",
        "\n",
        "experiment managers, such as Sacred, build your own tools for your own experiments. \n",
        "\n",
        "keep iterating model development\n",
        "\n",
        "Finally, think about how do you deploy this model.\n",
        "Deploy this with client facing user input through an app.\n",
        "\n",
        "How do you display this projec tot someone and make it look nice. \n",
        "\n",
        "Simple interface so that they can get predictions for the model. Think about the context , what the model does. \n",
        "Existing tools exist. Think about simple servers, where a user will serve a request, run the model, and then serve the prediction. Just in the http request. You don't even have an interface, you can just have a simple API\n",
        "\n",
        "read about how models get deployed at inference time. Look at AWS EC2 and lambda. How do you deal with data. do you persist those user requests, and things like.\n",
        "Read into flask.\n",
        "\n",
        "set up a siimple server that can handle requests. How to launch an ec2 (hosting servers) instqance and how to store data in an s3 bucket. Dynamo for databases. Lambda for inference that is adaptable.\n",
        "\n",
        "GCP, AWS. server.\n",
        "\n",
        "Think about presentation."
      ]
    }
  ]
}