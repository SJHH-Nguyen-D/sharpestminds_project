{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_sm_minified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvMEjvGpXr90",
        "colab_type": "text"
      },
      "source": [
        "## Installation of Packages and other settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE8-IdAuiE6V",
        "colab_type": "code",
        "outputId": "194063d2-14d1-4fd1-cc0d-08b1e1a750c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "!--NotebookApp.iopub_data_rate_limit=1e11\n",
        "!pip install pandas-profiling && pip install mlxtend && pip install missingpy && !pip install tpot"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: --: invalid option\n",
            "Usage:\t/bin/bash [GNU long option] [option] ...\n",
            "\t/bin/bash [GNU long option] [option] script-file ...\n",
            "GNU long options:\n",
            "\t--debug\n",
            "\t--debugger\n",
            "\t--dump-po-strings\n",
            "\t--dump-strings\n",
            "\t--help\n",
            "\t--init-file\n",
            "\t--login\n",
            "\t--noediting\n",
            "\t--noprofile\n",
            "\t--norc\n",
            "\t--posix\n",
            "\t--rcfile\n",
            "\t--restricted\n",
            "\t--verbose\n",
            "\t--version\n",
            "Shell options:\n",
            "\t-ilrsD or -c command or -O shopt_option\t\t(invocation only)\n",
            "\t-abefhkmnptuvxBCHP or -o option\n",
            "Requirement already satisfied: pandas-profiling in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (3.1.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (0.25.2)\n",
            "Requirement already satisfied: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (2.10.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (1.17.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pandas-profiling) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling) (41.4.0)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (41.4.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->mlxtend) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.17.1->mlxtend) (1.12.0)\n",
            "Collecting missingpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/be/998d04d27054b58f0974b5f09f8457778a0a72d4355e0b7ae877b6cfb850/missingpy-0.2.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.9MB/s \n",
            "\u001b[?25hInstalling collected packages: missingpy\n",
            "Successfully installed missingpy-0.2.0\n",
            "/bin/bash: !pip: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP6jx4qqX5Rw",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcK9VpumW216",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.options.display.max_seq_items = 2000\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas_profiling\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chxHr0tsIKyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# categorical_df.loc[categorical_df.v8.isnull(), \"v8\"].index[:10]\n",
        "# numeric_df[numeric_df.writhome.isnull()].index[:10]\n",
        "# missing_writhome = list(numeric_df[numeric_df.writhome.isnull()].index[:10])\n",
        "# imputed_df.loc[missing_writhome, \"writhome\"].head()\n",
        "# print(f\"writhome attribute has: {imputed_df.writhome.isnull().sum()} missing values\")\n",
        "# print(imputed_df.writhome.agg(['median', 'var', 'std', 'kurt', 'skew']))\n",
        "# print(imputed_df.writhome.describe())\n",
        "# print(impute_df[df.writhome.isnull()].head())\n",
        "# missing_cnt_h = list(categorical_df[categorical_df.cnt_h.isnull()].index)\n",
        "# imputed_df.loc[missing_cnt_h, \"cnt_h\"].head()\n",
        "# print(f\"cnt_h attribute has: {imputed_df.cnt_h.isnull().sum()} missing values\")\n",
        "# print(imputed_df.cnt_h.agg(['median', 'var', 'std', 'kurt', 'skew']))\n",
        "# print(imputed_df.cnt_h.describe())\n",
        "# print(impute_df[df.writhome.isnull()].head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne7BCf2oOMQE",
        "colab_type": "text"
      },
      "source": [
        "## Mappings and Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW9yFhg2OPba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DRIVENAME = '/content/drive/'\n",
        "FILENAME = \"/content/drive/My Drive/sharpestminds_dataset/hw5-trainingset-cl3770.csv\"\n",
        "\n",
        "CONSIDERED_MISSING_VALUES = ['999', 9995, '9995', 9996, '9996', 9997, \"9997\", 9998, '9998', 9999, '9999', '99999']\n",
        "REDUNDANT_FEATURES = [\"readytolearn_wle_ca\", \"icthome_wle_ca\", \"ictwork_wle_ca\", \n",
        "                      \"influence_wle_ca\", \"planning_wle_ca\", \"readhome_wle_ca\", \n",
        "                      \"readwork_wle_ca\", \"taskdisc_wle_ca\", \"writhome_wle_ca\", \n",
        "                      \"writwork_wle_ca\", \"ageg10lfs\", \"ageg10lfs_t\", \"edcat7\", \n",
        "                      \"edcat8\", \"isco2c\", \"isic2c\", \"isic2l\",\"earnflag\", \"reg_tl2\", \"lng_bq\", \n",
        "                      \"lng_ci\", \"edlevel3\", \"nfehrsnjr\", \"nfehrsjr\", \"nfe12jr\", \n",
        "                      \"fnfe12jr\", \"fnfaet12jr\", \"faet12jr\", \"faet12njr\", \"fe12\", \"monthlyincpr\",\n",
        "                      \"earnhrdcl\", \"earnhrbonusdcl\", \"row\", \"uni\", \"cntryid_e\", \"v270\", \n",
        "                      \"v205\", \"neet\", \"v84\", \"nfe12njr\", \"fnfaet12njr\"\n",
        "                     ]\n",
        "\n",
        "ORDINAL_VARIABLE_MAPPING = [\n",
        "    [[\"v233\", \"v280\", \"v103\", \"v15\", \"v24\", \"v108\", \"v218\", \"v171\", \"v189\", \n",
        "     \"v204\", \"v166\", \"v267\", \"v292\", \"v155\", \"v165\", \"v190\", \"v288\", \n",
        "     \"v276\",\"v43\", \"v197\", \"v214\", \"v7\", \"v175\", \"v139\", \"v123\", \"v14\", \"v178\",\n",
        "    \"v34\", \"v106\", \"v246\", \"v131\", \"v111\", \"v173\", \"v260\", \"v164\", \"v186\", \"v240\", \"v208\",\n",
        "    \"v275\", \"v132\", \"v141\", \"v25\", \"v177\", \"v149\", \"v23\", \"v193\", \"v237\", \"v162\", \"v146\",\n",
        "    \"v277\", \"v40\", \"v73\", \"v195\"], \n",
        "    ['Never', 'Less than once a month','Less than once a week but at least once a month','At least once a week but not every day','Every day']],\n",
        "    [['v244', \"v65\", \"v263\", \"v158\", \"v57\", \"v170\", \"v198\", \"v191\", \"v114\", \"v27\"], ['Not at all', 'Very little', 'To some extent', 'To a high extent','To a very high extent']], \n",
        "    [[\"v151\"], ['Aged 15 or younger', 'Aged 16-19', 'Aged 20-24', 'Aged 25-29','Aged 30-34', 'Aged 35 or older']],\n",
        "    [[\"v181\"], ['Extremely dissatisfied', 'Dissatisfied', 'Neither satisfied nor dissatisfied', 'Satisfied', 'Extremely satisfied']],\n",
        "    [[\"v271\"], ['Straightforward','Moderate','Complex']], \n",
        "    [[\"v247\", \"v134\", \"v13\", \"v18\", \"v26\", \"v124\", \"v99\", \"v282\", \"v51\", \"v2\", \"v229\", \"v248\"], ['Never','Rarely or never', 'Rarely','Less than once a week', \n",
        "                                                                                         'Less than once a week but at least once a month' ,'At least once a week']],\n",
        "    [[\"v85\", \"v50\", \"v69\"], ['Strongly disagree', 'Disagree', 'Neither agree nor disagree', 'Agree', 'Strongly agree']],\n",
        "    [[\"v291\", \"v77\"], ['None of the time', 'Up to a quarter of the time','Up to half of the time','More than half of the time','All of the time']],\n",
        "    [[\"v269\"], ['Not useful at all', 'Somewhat useful' , 'Moderately useful','Very useful']],\n",
        "    [[\"v216\"], ['Rarely or never','Less than once a week', 'At least once a week']],\n",
        "    [[\"v253\", \"v278\", \"v284\"], ['Never', 'Rarely', 'Less than once a month', 'Less than once a week but at least once a month', \n",
        "                        'At least once a week', 'At least once a week but not every day', 'Every day']],\n",
        "    [[\"ageg5lfs\"], ['Aged 16-19','Aged 20-24','Aged 25-29','Aged 30-34','Aged 35-39', 'Aged 40-44', 'Aged 45-49','Aged 50-54','Aged 55-59','Aged 60-65']],\n",
        "    [[\"v289\"], ['No income', 'Lowest quintile','Next lowest quintile','Mid-level quintile', 'Next to highest quintile' ,'Highest quintile']],\n",
        "    [[\"v261\"], ['0 - 20 hours','21 - 40 hours', '41 - 60 hours' , '61 - 80 hours', '81 - 100 hours', 'More than 100 hours']],\n",
        "    [[\"v221\"], ['None','Less than 1 month','1 to 6 months','7 to 11 months', '1 or 2 years','3 years or more']],\n",
        "    [[\"v82\", \"v70\"], ['Self-employed or unpaid family worker', 'Employee, not supervisor', 'Self-employed, not supervisor',\n",
        "                      'Employee, supervising fewer than 5 people', 'Employee, supervising more than 5 people', 'Self-employed, supervisor']],\n",
        "    [[\"v200\"], ['Not definable', 'Less than high school', 'High school', 'Above high school']],\n",
        "    [[\"v62\"], ['A higher level would be needed', 'This level is necessary', 'A lower level would be sufficient']],\n",
        "    [[\"v236\"], ['No, not at all', 'There were no such costs', 'No employer or prospective employer at that time' ,'Yes, partly', 'Yes, totally']],\n",
        "    [[\"v19\"], ['Aged 19 or younger', 'Aged 20-24', 'Aged 25-29', 'Aged 30-34' ,'Aged 35-39' ,'Aged 40-44', 'Aged 45-49', 'Aged 50-54', 'Aged 55 or older']],\n",
        "    [[\"imyrcat\"], ['In host country 5 or fewer years', 'In host country more than 5 years', 'Non-immigrants']],\n",
        "    [[\"v48\"], ['1 to 10 people', '11 to 50 people', '51 to 250 people', 'More than 1000 people', '251 to 1000 people']],\n",
        "    [[\"v47\"], ['Days', 'Weeks',  'Hours']],\n",
        "    [[\"iscoskil4\"], ['Elementary occupations', 'Skilled occupations','Semi-skilled blue-collar occupations', 'Semi-skilled white-collar occupations']],\n",
        "    [[\"v94\"], ['Respondent reported no learning activities', 'Respondent reported 1 learning activity', \n",
        "               'Respondent reported learning activities but number is not known', 'Respondent reported more than 1 learning activity']],\n",
        "    [[\"v8\"], ['Decreased', 'Stayed more or less the same', 'Increased']],\n",
        "    [['edcat6'], ['Lower secondary or less (ISCED 1,2, 3C short or less)\\xa0',\n",
        "                'Upper secondary (ISCED 3A-B, C long)',\n",
        "                'Post-secondary, non-tertiary (ISCED 4A-B-C)',\n",
        "                'Tertiary – bachelor degree (ISCED 5A)',\n",
        "                'Tertiary - bachelor/master/research degree (ISCED 5A/6)',\n",
        "                'Tertiary – master/research degree (ISCED 5A/6)',\n",
        "                'Tertiary – professional degree (ISCED 5B)']]\n",
        "]\n",
        "\n",
        "BINARY_VARIABLE_MAPPING = {\n",
        "    \"gender_r\": {'Male': 0, 'Female': 1},\n",
        "    \"faet12\": {'Did not participate in formal AET': 0, 'Participated in formal AET': 1},\n",
        "    \"v46\" : {'One job or business': 0, 'More than one job or business': 1},\n",
        "    \"v53\" : {'Employee': 0, 'Self-employed': 1},\n",
        "    \"nfe12\" : {'Did not participate in NFE': 0, 'Participated in NFE': 1},\n",
        "    \"nativelang\" : {'Did not participate in NFE': 0, 'Participated in NFE': 1},\n",
        "    \"nopaidworkever\": {\"Has not has paid work ever\": 0, \"Has had paid work\": 1},\n",
        "    \"paidwork5\" : {\"Has not had paid work in past 5 years\": 0, \"Has had paid work in past 5 years\": 1},\n",
        "    \"paidwork12\" : {\"Has not had paid work during the 12 months preceding the survey\": 0, \"Has had paid work during the 12 months preceding the survey\": 1},\n",
        "    \"aetpop\" : {\"Excluded from AET population\": 0, \"AET population\": 1},\n",
        "    # \"edwork\" : {\"In work only\": 0, \"In education and work\": 1},\n",
        "    # \"v122\" : {'Yes, unpaid work for family business': 0, 'Yes, paid work one job or business': 1, 'Yes, paid work more than one job or business or number of jobs/businesses missing': 2},\n",
        "    \"nativelang\": {'Test language not same as native language': 0, 'Test language same as native language':1},\n",
        "    \"fnfaet12\": {'Did not participate in formal or non-formal AET': 0, 'Participated in formal and/or non-formal AET': 1}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ86NHkaOHiO",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTD8w9OeOKCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_outliers_and_extremes(df, numeric_attribute='job_performance'):\n",
        "    from scipy.stats import iqr\n",
        "    from numpy import percentile\n",
        "\n",
        "    IQR = iqr(df[numeric_attribute], axis=0, rng=(25, 75), scale='raw', nan_policy='propagate', interpolation='linear', keepdims=False)\n",
        "    q1 = percentile(df[numeric_attribute], 0.25, axis=0, out=None, overwrite_input=False, interpolation='linear', keepdims=False)\n",
        "    q3 = percentile(df[numeric_attribute], 0.75, axis=0, out=None, overwrite_input=False, interpolation='linear', keepdims=False)\n",
        "    \n",
        "    outliers = df[(df[numeric_attribute] <= (q1 - (IQR * 1.5))) | (df[numeric_attribute] <= (q3 + (IQR * 1.5)))]\n",
        "    extremes = df[(df[numeric_attribute] <= (q1 - (IQR * 1.5))) | (df[numeric_attribute] <= (q3 + (IQR * 1.5)))]\n",
        "    return outliers\n",
        "\n",
        "\n",
        "def detect_highly_correlated_variables(dataframe):\n",
        "    \"\"\" Uses the pandas profiler to detect the highly correlated variables to drop \"\"\"\n",
        "    import pandas_profiling\n",
        "    profile = pandas_profiling.ProfileReport(dataframe)\n",
        "    rejected_variables = profile.get_rejected_variables(threshold=0.9)\n",
        "    return rejected_variables\n",
        "\n",
        "\n",
        "def df_by_type_splitter(dataframe):\n",
        "    \"\"\" a larger dataframe into immediately identifiable numeric and other type dataframes\"\"\"\n",
        "    num_df = dataframe._get_numeric_data().copy()\n",
        "    cat_df = dataframe.select_dtypes(exclude = [int, float]).copy()\n",
        "    return num_df, cat_df\n",
        "\n",
        "\n",
        "def binary_variable_mapping(dataframe, mapping_dict):\n",
        "    \"\"\" mapping of numeric values to binary outcomes \"\"\"\n",
        "    # yes and no mappings\n",
        "    yes_no_mapping = {'Yes': 1, 'No': 0}\n",
        "    for feature in dataframe.columns:\n",
        "        if \"Yes\" in dataframe[feature].unique():\n",
        "            dataframe[feature] = dataframe[feature].replace(yes_no_mapping)\n",
        "\n",
        "    for feature_name, mapping in mapping_dict.items():\n",
        "            dataframe[feature_name] = dataframe[feature_name].replace(mapping)\n",
        "\n",
        "\n",
        "def ordinal_variable_mapping(dataframe, mapping):\n",
        "    for feats, cat in mapping:\n",
        "        for att in feats:\n",
        "            indiv_feat_mapping = {key: val for val, key in enumerate(cat)}\n",
        "            dataframe[att].replace(to_replace=indiv_feat_mapping, inplace=True)\n",
        "\n",
        "\n",
        "def nominal_feature_mapping(dataframe):\n",
        "    \"\"\"transform mapping for nominal features\"\"\"\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    nominal_categorical_encoding_manifest = {}\n",
        "    # temp fill of NaN values with a string\n",
        "    dataframe.fillna('Null', inplace=True)\n",
        "    for col in dataframe.columns:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(dataframe[col].values.ravel())\n",
        "        dataframe[col] = le.transform(dataframe[col].values.ravel())\n",
        "        nominal_categorical_encoding_manifest[col] = list(le.classes_)\n",
        "        if dataframe[col].isnull().sum() > 0:\n",
        "            # fill back temp-fill \"Null\" encoded values with actual NaN values for later imputer\n",
        "            dataframe[col].replace(to_replace=list(le.classes_).index('Null'), value=np.nan, inplace=True)\n",
        "            null_index = list(le.classes_).index('Null')\n",
        "        le = None\n",
        "\n",
        "\n",
        "def transform_all(dataframe, binary_mapping, ordinal_mapping):\n",
        "    \"\"\"all transformations into one function\"\"\"\n",
        "    # binary mappings\n",
        "    binary_feature_names = list(set([col for col in categorical_df.columns if len(categorical_df[col].unique()) <= 3]) - set([\"v51\", \"v229\", \"v13\"]))\n",
        "    binary_df = dataframe[binary_feature_names]\n",
        "    binary_variable_mapping(binary_df, binary_mapping)\n",
        "    \n",
        "    # ordinal mappings\n",
        "    ordinal_feature_names = [\n",
        "    \"v233\", \"v280\", \"v103\", \"v15\", \"v24\", \"v108\", \"v218\", \"v171\", \"v189\",\n",
        "     \"v204\", \"v166\", \"v267\", \"v292\", \"v155\", \"v165\", \"v190\", \"v288\",\n",
        "     \"v276\",\"v43\", \"v197\", \"v214\", \"v7\", \"v175\", \"v139\", \"v123\", \"v14\", \n",
        "     \"v178\", \"v34\", \"v106\", \"v246\", \"v131\", \"v111\", \"v173\", \"v260\", \"v164\", \n",
        "     \"v186\", \"v240\", \"v208\", \"v275\", \"v132\", \"v141\", \"v25\", \"v177\", \"v149\", \n",
        "     \"v23\", \"v193\", \"v237\", \"v162\", \"v146\", \"v277\", \"v40\", \"v73\", \"v195\", 'v244',\n",
        "     \"v65\", \"v263\", \"v158\", \"v57\", \"v170\", \"v198\", \"v278\", \"v191\", \"v114\", \"v27\", \n",
        "     \"v151\", \"v181\", \"v271\", \"v247\", \"v134\", \"v13\", \"v18\", \"v26\", \"v124\", \"v99\", \n",
        "     \"v282\", \"v51\", \"v2\", \"v229\", \"v248\",\"v291\", \"v77\",\"v269\", \"v216\",\"v253\", \n",
        "     \"v284\", \"ageg5lfs\", \"v289\", \"v261\", \"v221\", \"v85\",\"v50\",\"v69\", \"v82\", \n",
        "     \"v70\", \"v200\", \"v62\", \"v236\",\"v19\", \"imyrcat\",\"v48\",\"v47\",\"iscoskil4\",\"v94\",\n",
        "     \"v8\",'edcat6',]\n",
        "    ordinal_df = dataframe[ordinal_feature_names]\n",
        "    ordinal_variable_mapping(ordinal_df, ordinal_mapping)\n",
        "\n",
        "    # nominal encoding\n",
        "    nominal_feature_names = [\"cntryid\", \"lng_home\", \"cnt_h\", \"cnt_brth\", \n",
        "                             \"ctryqual\", \"birthrgn\", \"ctryrgn\", \"isic1c\",\n",
        "                             \"isic1l\", \"v31\", \"v137\", \"v234\", \"v91\",\"v92\",\n",
        "                             \"v88\", \"v140\", \"v3\",]\n",
        "    nominal_df = dataframe[nominal_feature_names]\n",
        "    nominal_feature_mapping(nominal_df)\n",
        "    \n",
        "    # combine all\n",
        "    transformed_dataframe = pd.concat([binary_df, ordinal_df, nominal_df], axis=1)\n",
        "    return transformed_dataframe\n",
        "\n",
        "\n",
        "def impute_missing_for_dataframe(dataframe, target='job_performance'):\n",
        "    \"\"\" The imputer function should be used on a dataframe that has already been numerically encoded \"\"\"\n",
        "    from missingpy import KNNImputer #, MissForest\n",
        "    \n",
        "    X = dataframe.loc[:, dataframe.columns != target].values\n",
        "    y = dataframe[target].values\n",
        "\n",
        "    # imputer object\n",
        "    knn = KNNImputer(n_neighbors=5, \n",
        "                    weights=\"uniform\",\n",
        "                    metric=\"masked_euclidean\",\n",
        "                    row_max_missing=0.8,\n",
        "                    col_max_missing=0.8, \n",
        "                    copy=True)\n",
        "    knn_missing_imputation = knn.fit_transform(X)\n",
        "    imputed_dataframe = pd.DataFrame(knn_missing_imputation, \n",
        "                                     columns = dataframe.columns[dataframe.columns != target])\n",
        "    imputed_dataframe[target] = pd.Series(y)\n",
        "    return imputed_dataframe\n",
        "\n",
        "\n",
        "def round_selected_attributes_imputed(dataframe_to_round, dataframe_not_round):\n",
        "    rounded_dataframe = dataframe_to_round.apply(lambda x: x.round())\n",
        "    dataframe = pd.concat([rounded_dataframe, dataframe_not_round], axis=1).reset_index()\n",
        "    # dataframe.drop(\"index\", axis=1, inplace=True)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def split_dataframe(dataframe, target=\"job_performance\", test_size=0.3, random_state=123):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataframe.loc[:, dataframe.columns != target].values,\n",
        "        dataframe[target].values.ravel(),\n",
        "        test_size=test_size,\n",
        "        random_state=random_state)\n",
        "    y_train = y_train.ravel()\n",
        "    y_test = y_test.ravel()\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def select_n_features(X, Y, n_features=10):\n",
        "    \"\"\" uses the mlxtend module to select a number of features to keep in the dataframe \"\"\"\n",
        "    from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "    # # Build RF regressor to use in feature selection\n",
        "    rfr = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
        "\n",
        "    sfs = SFS(rfr, \n",
        "              k_features=n_features, \n",
        "              forward=True, \n",
        "              floating=False, \n",
        "              scoring='r2',\n",
        "              n_jobs=-1,\n",
        "              cv=10)\n",
        "\n",
        "    sfs = sfs.fit(X, Y)\n",
        "\n",
        "    feature_indices = sfs.k_feature_idx_\n",
        "    feature_names = sfs.k_feature_names_\n",
        "\n",
        "    return feature_indices, feature_names\n",
        "\n",
        "# Train/test split using the new 10 feature selected dataset\n",
        "def model_selection_and_HPO(dataframe, target=\"job_performance\", test_size=0.25, r_seed=123):\n",
        "    \"\"\" Pass in the dataframe that has gone through feature selection\n",
        "    Uses the TPOT regressor module from TPOT to perform MS and HPO. As this modeling uses some element\n",
        "    of stochasticity, it may provide different results every time. The longer you run this,\n",
        "    the more similar the final models will look like in the end.\n",
        "    \n",
        "    Finally outputs a .py file with the selected model and its hyperparameters, for which we can import.\n",
        "    \"\"\"\n",
        "    import TPOT \n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import timeit\n",
        "    from tpot import TPOTRegressor\n",
        "    from sklearn.metrics import (\n",
        "        confusion_matrix,\n",
        "        roc_auc_score,\n",
        "        precision_recall_fscore_support,\n",
        "        accuracy_score,\n",
        "    )\n",
        "\n",
        "    # train test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataframe.loc[:, dataframe.columns != target].values,\n",
        "        dataframe[target].values.ravel(),\n",
        "        test_size=test_size,\n",
        "        random_state=r_seed)\n",
        "    \n",
        "    y_train = y_train.ravel()\n",
        "    y_test = y_test.ravel()\n",
        "\n",
        "    # model selection and hyperparameter optimization with TPOT Regressor\n",
        "    tpot_regressor = TPOTRegressor(generations=20, \n",
        "                                   population_size=50, \n",
        "                                   cv=10,\n",
        "                                   random_state=r_seed, \n",
        "                                   verbosity=2, \n",
        "                                   memory='auto')\n",
        "    \n",
        "    start_time = timeit.default_timer()\n",
        "    tpot_regressor.fit(X_train, y_train)\n",
        "    y_pred = tpot_regressor.predict(X_test)\n",
        "    end_time = timeit.default_timer()\n",
        "\n",
        "    print(f\"Total runtime for the Employee dataset: {end_time-start_time}s\")\n",
        "    print(\"TPOT Score: {}\".format(tpot_regressor.score(X_test, y_test)))\n",
        "\n",
        "    tpot_regressor.export('tpot_exported_pipeline.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbJJcJ7lifKc",
        "colab_type": "text"
      },
      "source": [
        "## Run All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1Y4_ViMIGt",
        "colab_type": "code",
        "outputId": "adeb789c-1756-4afb-8265-2abd82e34643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "drive.mount(DRIVENAME)\n",
        "df = pd.read_csv(FILENAME, header='infer')\n",
        "df = (df.drop(index=get_outliers_and_extremes(df, numeric_attribute='job_performance').index, inplace=False, axis=0) # drop outliers\n",
        "        .drop(labels=detect_highly_correlated_variables(df), inplace=False, axis=1) # drop highly related correlated variables\n",
        "        .replace(to_replace=CONSIDERED_MISSING_VALUES, value=np.nan, inplace=False) # missing value encoding\n",
        "        .drop(labels=list(set([feature for feature in df.columns if (df[feature].isnull().sum(axis=0) / df.shape[0]) >= 0.6])-set(detect_highly_correlated_variables(df))),\n",
        "             inplace=False, \n",
        "             axis=1))\n",
        "df = (df.drop(index=list(df[((df.isnull().sum(axis=1)/df.shape[1]) >= 0.40) == True].index), # drop rows that have more than 40% of values missing\n",
        "             inplace=False,\n",
        "             axis=0) # drop observations with greater than 40% of values missing\n",
        "       .drop(labels=REDUNDANT_FEATURES, inplace=False, axis=1)) # drop redundant features\n",
        "\n",
        "numeric_df, categorical_df = df_by_type_splitter(df)\n",
        "categorical_df = transform_all(categorical_df, BINARY_VARIABLE_MAPPING, ORDINAL_VARIABLE_MAPPING)\n",
        "\n",
        "df = (pd.concat([numeric_df, categorical_df], axis=1)\n",
        "        .drop(labels=list(numeric_df.loc[:, ((numeric_df.isnull().sum(axis=0) / numeric_df.shape[0]) >= 0.6)].columns), \n",
        "              inplace=False, axis=1))\n",
        "\n",
        "# Imputation of Missing Features\n",
        "start_time = default_timer()\n",
        "df = impute_missing_for_dataframe(df, target=\"job_performance\")\n",
        "df = round_selected_attributes_imputed(df[categorical_df.columns], df[[col for col in df.columns if col not in categorical_df.columns]])\n",
        "end_time = default_timer()\n",
        "print(\"Impute time: {}\".format(end_time - start_time))\n",
        "\n",
        "# Feature Selection\n",
        "start_time = default_timer()\n",
        "X_train, X_test, y_train, y_test = split_dataframe(df, target=\"job_performance\", test_size=0.3, random_state=123)\n",
        "feature_indices, feature_names = select_n_features(X_train, y_train, n_features=round(0.33*len(df.columns)-1))\n",
        "end_time = default_timer()\n",
        "print(\"Feature Selection Time: {}\".format(end_time - start_time))\n",
        "\n",
        "# Model Selection\n",
        "start_time = default_timer()\n",
        "model_selection_and_HPO(df[feature_names], test_size=0.3)\n",
        "end_time = default_timer()\n",
        "print(\"Model Selection Time: {}\".format(end_time - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
            "  DeprecationWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:432: DeprecationWarning: 'warn_on_dtype' is deprecated in version 0.21 and will be removed in 0.23. Don't set `warn_on_dtype` to remove this warning.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Impute time: 152.62876166800015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm1lrl5dSU_A",
        "colab_type": "text"
      },
      "source": [
        "## SANDBOX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OIhibAqmCj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# have to fix a piece of the drop pipe line where it does not calculate percentagee missing by columns properly \n",
        "# this affects the imputation sequence and throws an error\n",
        "drive.mount(DRIVENAME)\n",
        "df = pd.read_csv(FILENAME, header='infer')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}