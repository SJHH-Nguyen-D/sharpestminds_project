useful to learn about how to scale, ingest, store and query the data through a data pipeline.

what are the tools and the tradeoffs between these tools.

finding a good workflow is important.

preferences for tooling, environment, etc.

# interview questions about charlie's current job
* how to debug a neural network
* train enough models to know where the commnon plces run into bugs
* tuning hyperparameters, regularization
* onsite interview:
	* hour long presentation on the research
	* break into three sessions with technical ML questions
	* general ML questions
	* 1st session has an applied problem and see how you would work through it and ask the right questions, and make the valid assumptions
	* the 2nd interview had general ML stuff (KL divergence, what is random forest), git, docker
	* the 3rd interview was more theory based. Why is the dice coefficient not differentiable, coming up with new loss methods, receptive fields, how does activations vary with depth in a unit.

